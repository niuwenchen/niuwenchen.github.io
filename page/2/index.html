<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="SpeechAndLanguageProcessing">
<meta property="og:url" content="http://niuwenchen.github.io/page/2/index.html">
<meta property="og:site_name" content="SpeechAndLanguageProcessing">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SpeechAndLanguageProcessing">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://niuwenchen.github.io/page/2/"/>





  <title>SpeechAndLanguageProcessing</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SpeechAndLanguageProcessing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">translate and learning language model</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Seq2Seq中的函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Seq2Seq中的函数/" itemprop="url">Seq2Seq中的函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T14:28:46+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Seq2Seq 中的函数</h2>
<pre><code>tf.constant(value): 将一个常数转化为tensor
tf.cast(x,dtype): 将x的数据格式转化成dtype

a = tf.Variable([1,0,0,1,1])
b = tf.cast(a,dtype=tf.bool)
sess = tf.InteractiveSession()
sess.run(tf.initialize_all_variables())
print(sess.run(b))
[ True False False  True  True]



1. src_vocab_table.lookup(tf.constant(eos))


2. tf.contrib.lookup.index_table_from_tensor(),构建单词索引对
	
	mapping_strings = tf.constant([&quot;emerson&quot;, &quot;lake&quot;, &quot;palmer&quot;])
	table = tf.contrib.lookup.index_table_from_tensor(
  	mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
	features = tf.constant([&quot;emerson&quot;, &quot;lake&quot;, &quot;and&quot;, &quot;palmer&quot;])
	ids = table.lookup(features)

	with tf.Session() as sess:
		tf.tables_initializer().run()
		print(ids.eval() )
		[0 1 3 2]
</code></pre>
<p>Dataset 与Iterator</p>
<pre><code>Dataset可以看作是相同类型元素的有序列表，在实际使用时，单个元素可以是向量，也可以是字符串，图片，tuple或者dict

是TextLineDataset TFRecordDataset FixedLengthRecorddataset 的父类

dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))
这个dataset中含有5个元素，分别是1.0, 2.0, 3.0, 4.0, 5.0。

如何取出这个dataset中的元素？方法是从dataset中实例化一个Iterator，然后对Iterator进行迭代

iterator = dataset.make_one_shot_iterator()
one_element = iterator.get_next()
with tf.Session() as sess:
	for i in range(5):
    	print(sess.run(one_element))

对DataSet中的元素做变换: Transformation

map:  dataset.map(lambda x:x+1)
batch: dataset.batch(32) 将每个元素组成了大小为32的batch
	batch的意思是将原始数据分成每个batch大小为batch_size的数据

shuffle: dataset.shuffle(buffer_size=10000),表示打乱时使用的buffer大小

repeat: 将整个序列重复多次，主要处理机器学习中的epoch，假设原先的数据是一个epoch，使用repeat(5)之后编程5个epoch

dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0,6.0]))
dataset = dataset.map(lambda x:x*2)
dataset = dataset.shuffle(buffer_size=10)
dataset = dataset.batch(3)
iterator = dataset.make_one_shot_iterator()
one_element = iterator.get_next()
with tf.Session() as sess:
	print(sess.run(one_element))
	print(sess.run(one_element))

[  6.   2.  10.]
[  8.   4.  12.]

如果不调用shuffle，那么repeat的是一样的数据
[ 2.  4.  6.]
[  8.  10.  12.]
[ 2.  4.  6.]
[  8.  10.  12.]


其他Iterator
initializable_iterator:必须在使用前通过sess.sun()来初始化，使用initializble_iterator ，可以将placeholder带入iterator中，可以方便通过参数快读定义Iterator

dataset = tf.data.Dataset.from_tensor_slices(tf.range(start=0, limit=limit))

iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
	sess.run(iterator.initializer, feed_dict={limit: 10})
	for i in range(10):
  		value = sess.run(next_element)
  		assert i == value
</code></pre>
<p>tf.string_split(source,delimiter=' ',skip_empty=True)</p>
<pre><code>string=[&quot;hello world Jack niu&quot;]
sp=tf.string_split(string)

with tf.Session() as sess:
	sess.run(tf.global_variables_initializer())
	print(sess.run(sp).values)

[b'hello' b'world' b'Jack' b'niu']


dataset = tf.data.Dataset.from_tensor_slices(
        tf.constant([&quot;c c a&quot;, &quot;c a&quot;, &quot;d&quot;, &quot;f e a g&quot;]))

# src_dataset = src_dataset.map(lambda  x:tf.string_split([x]).values)
dataset = dataset.map(lambda x: tf.string_split([x]).values)
src_max_len=2
dataset = dataset.map(lambda x: x[:src_max_len])
dataset = dataset.map(lambda src: (src, tf.size(src)))
iterator = dataset.make_one_shot_iterator()


one_element = iterator.get_next()
with tf.Session() as sess:
	print(sess.run(one_element))
	print(sess.run(one_element))
	print(sess.run(one_element))
	print(sess.run(one_element))

(array([b'c', b'c'], dtype=object), 2)
(array([b'c', b'a'], dtype=object), 2)
(array([b'd'], dtype=object), 1)
(array([b'f', b'e'], dtype=object), 2)
</code></pre>
<p>padded_batch:</p>
<pre><code>dataset =dataset.padded_batch(batch_size,padded_shape,padded_value)

许多模型（比如：序列模型）的输入数据的size多种多样（例如：序列具有不同的长度）为了处理这种情况，Dataset.padded_batch() 转换允许你将不同shape的tensors进行batch，通过指定一或多个dimensions，在其上进行pad

将顺序的元素组装成batch，
(array([b'c', b'c', b'a'], dtype=object), 3)
(array([b'c', b'a'], dtype=object), 2)
.....

这样的单一顺序元素
现在进行padded_batch(3,padded_shape=(tf.TensorShape[None],tf.TensorShape([])),padded_values=(&quot;UNK&quot;,0))

(array([[b'c', b'c', b'a'],
   [b'c', b'a', b'UNK'],
   [b'd', b'UNK', b'UNK']], dtype=object), array([3, 2, 1]))
(array([[b'f', b'e', b'a']], dtype=object), array([3]))

现在所有batch的长度都一样，就将所有长度不一的元素组成了一个batch内长度相同的元素。这样对这一个batch内的元素可以进行训练。

这种填充会对每一部分都进行pad，如果不进行pad，那就shape=[],value=0
</code></pre>
<p>示例：</p>
<pre><code>import tensorflow as tf

dataset = tf.contrib.data.Dataset.range(100)
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))
dataset = dataset.padded_batch(32, padded_shapes=[None],padding_values=(tf.cast(-1,tf.int64)))

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

sess= tf.Session()
print(sess.run(next_element))  # ==&gt; [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]
print(sess.run(next_element)[0])     # print(sess.run(one_element))

[31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31]]
[32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]
</code></pre>
<p>tf.logical_and(tf.size(src) &gt; 0, tf.size(tgt) &gt; 0))</p>
<p>src_tgt_dataset = src_tgt_dataset.shard(2, 1)</p>
<pre><code>shard: 将一个数据集划分成1/num_shard 的数据子集
索引代表需要哪一块数据。
</code></pre>
<p>padd</p>
<pre><code>数据是
(array([-1, -1,  0]), array([4, 2, 2]), array([2, 2, 3]))
(array([2, 2, 0]), array([4, 0, 1]), array([0, 1, 3]))
(array([2, 0]), array([4, 1, 2]), array([1, 2, 3]))

这里的注释:
# Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)


batched_dataset = src_tgt_dataset.apply(
    tf.contrib.data.group_by_window(
        key_func=key_func, reduce_func=reduce_func, window_size=batch_size))
这个函数有点麻烦。
</code></pre>
<p>tf.container(scope or &quot;train&quot;)</p>
<pre><code>有什么作用
</code></pre>
<p>tf.clip_by_global_norm(gradients, max_gradient_norm)</p>
<p>model 中的build_decoder</p>
<pre><code>iterator = self.iterator
print(&quot;iterator.source_sequence_length&quot;,iterator.source_sequence_length)
# maximum_iteration: The maximum decoding steps.
maximum_iterations = self._get_infer_maximum_iterations(
    hparams, iterator.source_sequence_length)

[3 2]
3
6
maximum_iterations =  6
</code></pre>
<p>dynamic_rnn 中的time_major</p>
<pre><code>time_major: The shape format of the inputs and outputs 
Tensors. If true, these Tensors must be shaped 
[max_time, batch_size, depth]. If false, these Tensors 
must be shaped [batch_size, max_time, depth]. Using 
time_major = True is a bit more efficient because it 
avoids transposes at the beginning and end of the RNN 
calculation. However, most TensorFlow data is batch-
major, so by default this function accepts input and 
emits output in batch-major form.
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Seq2Seq训练过程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Seq2Seq训练过程/" itemprop="url">Seq2Seq训练过程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T14:22:06+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Seq2seq训练过程</h2>
<p>iterator_utils.py</p>
<pre><code>class BatchedInput(collections.namedtuple(
		&quot;BatchedInput&quot;,(&quot;initializer&quot;, &quot;source&quot;, &quot;target_input&quot;,
        &quot;target_output&quot;, &quot;source_sequence_length&quot;,
        &quot;target_sequence_length&quot;))):
pass
collection.namedtuple: 生成可以使用名字来访问元素内容的tuple子类


def get_infer_iterator(src_dataset,
                   src_vocab_table,
                   batch_size,
                   eos,
                   src_max_len=None):

(src_ids, src_seq_len) = batched_iter.get_next()
return BatchedInput(
    initializer=batched_iter.initializer,
    source=src_ids,
    target_input=None,
    target_output=None,
    source_sequence_length=src_seq_len,
    target_sequence_length=None)

src_ids: 训练语句src中word在词汇表中的id
src_seq_len: 训练语句src本身的长度。

训练数据:
encoder: [&quot;f e a g&quot;, &quot;c c a&quot;, &quot;d&quot;, &quot;c a&quot;]
decoder: [&quot;c c&quot;, &quot;a b&quot;, &quot;&quot;, &quot;b c&quot;]

encoder_input: [2 0 3] c a eos：用eos pad，实际长度是2
				[-1 -1 0] -1代表unknown，句子长度为4，实际为3

decoder_input: [[4,2,2],[4,1,2]] [[sos c c],[sos b c]]
				输入用sos开始

decoder_target: [[2,2,3],[1,2,3]],[[c c eos],[b c eos]]	输出用eos pad

看一下具体的构造过程

# Create a tgt_input prefixed with &lt;sos&gt; and 
# a tgt_output suffixed with &lt;eos&gt;.
src_tgt_dataset = src_tgt_dataset.map(
    lambda src, tgt: (src,
                      tf.concat(([tgt_sos_id], tgt), 0),
                      tf.concat((tgt, [tgt_eos_id]), 0)),
    num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)
</code></pre>
<p>vocab_utils</p>
<pre><code>下载数据，加载embedding数据，这种数据可以通过别的其他的完备的
训练语料库来决定。
现在在这里并没有加载embedding 数据。
</code></pre>
<p>nmt_utils</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Seq2Seq基本模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Seq2Seq基本模型/" itemprop="url">Seq2Seq基本模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T13:59:23+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Seq2Seq中的模型</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Attention-Model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Attention-Model/" itemprop="url">Attention Model</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T13:59:04+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/31/8-Neural-Networks-and-Neural/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/31/8-Neural-Networks-and-Neural/" itemprop="url">8 Neural Networks and Neural</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-31T09:56:40+08:00">
                2018-01-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Neural Networks and Neural Language Models</h2>
<p>本章将介绍许多其他方面的神经模型。</p>
<p>##Units
神经网络的最基本单元是一个计算单元。一个神经单元可以表示为</p>
<p>$$Z=b+ \sum_i w_{i}x_{i}$$</p>
<p>将求和过程替换为点乘过程</p>
<p>$$Z=w\cdot x+b$$</p>
<p>神经网络单元应用一个非线性函数给z。将这个函数的输出称为activation。</p>
<p>$$y=a=f(z)$$</p>
<p>三种非线性函数(sigmoid, tanh,relu)</p>
<p>sigmoid激活函数</p>
<p>$$y=\sigma (z) = \frac{1}{1+e^{-z}}$$</p>
<p>图像再不画，但是输出值的范围是[0,1]之间。对于输出限定在0和1之间的函数很有优势。但是这个函数在优化的过程中特别麻烦。</p>
<p><img src="/img/nlp8_0.png" alt=""></p>
<p><img src="/img/nlp8_1.png" alt=""></p>
<p>进行一个简单的例子</p>
<pre><code>w=[0.2,0.3,0.9]
b=0.5
x=[0.5,0.6,0.1]
</code></pre>
<p>$$y=\sigma (z) = \frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(0.5<em>0.2+0.3</em>0.6+0.1*0.8+0.5)}}=e^{-0.86}=0.42$$</p>
<p>tanh 激活函数，是sigmoid的变体,范围在-1 到1 之间</p>
<p>$$y=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$</p>
<p>最简单的激活函数relu函数</p>
<p>$$y=max(x,0)$$</p>
<p><img src="/img/nlp8_2.png" alt=""></p>
<p>这些激活函数有不同的属性，因此在不同的语言应用或者网络结构中有不同的使用方式。</p>
<h2>The XOR problem</h2>
<p>逻辑问题，首先出现在感知机中</p>
<p><img src="/img/nlp8_3.png" alt=""></p>
<pre><code>y:
0; if wx+b &lt;0
1; if wx=b &gt;0
</code></pre>
<p>可以用很简单的方式建立一个感知机</p>
<p><img src="/img/nlp8_4.png" alt=""></p>
<p>但是，没有办法建立一个感知机去解决逻辑XOR问题。</p>
<p>感知机是一个线性分类器。</p>
<p>。。。。。待补充</p>
<h2>Feed-Forward Neural Networks</h2>
<p>前馈神经网络。一个前向神经网络是一个多层网络但是没有环。前向神经网络也叫多层感知机。</p>
<p>最简单的前向神经网络如下图所示:</p>
<p><img src="/img/nlp8_5.png" alt=""></p>
<p>度量输出结果最好的方式是分类，常用的方式是归一化。softmax函数</p>
<p>$$y=softmax(x)$$</p>
<h2>Training Neutal Nets</h2>
<p>训练神经网络模型，就是对里面的参数进行调整，先对参数进行基本假设，然后在一个方向调整权重提高系统。</p>
<h3>Loss function</h3>
<p>神经网络的目标是定一个损失函数，然后寻找某种方式去最小化这个损失函数。
$$L(\hat{y},y)=How\ much\ \hat(y)\ differ\ from\ true\ y$$</p>
<p>损失函数---&gt;MSE(mean-squared error)</p>
<p>公式不再定义，就是误差平方均值。</p>
<p>多数情况下，我们考虑网络是一个概率分类。对于概率分类的误差函数是 cross entropy loss，或者叫做负对数似然(negative log likelihood)</p>
<p>Let y be a vector over the C classes representing the true output probability distribution. Assume this is a hard classification task,meaning that only one class is the correct one. If the true class is i, then y is a vector where yi=1 and yj=0. A vector like this,with one value =1 and the rest 0,is called one-hot vector. Noe let $\hat{y}$ be the vector output from the network. The loss is simply the log probability of the correct class：</p>
<p>$$L(\hat{y},y)= -logp(\hat{y})$$</p>
<h3>Following Gradients</h3>
<p>梯度下降方法
<img src="/img/nlp8_6.png" alt="">
<img src="/img/nlp8_7.png" alt=""></p>
<h3>Computing the Gradient</h3>
<p>计算梯度是用反向传播算法</p>
<h3>Stochastic Gradient Descent</h3>
<p><img src="/img/nlp8_8.png" alt="">
随机梯度下降之所以被称为随机是因为它选择一个随机样本，根据公式计算参数值。还有可选的算法是minibatch 梯度下降。</p>
<h2>Neural Language Models</h2>
<p>一个前向神经网络语言模型，时刻t的输入是一个前面的单词(wt-1,wt-2...)输出是一个对可能的下一个单词的概率分布。</p>
<p>$$P(w_{t}|w^{t-1}<em>1)\approx P(w</em>{t}|w^{t-1}_{t-N+1})$$
计算整个之前的上下文概率近似于计算之前的前N个单词的概率</p>
<p>如果我们使用一个4-gram，概率$P(w_{t}=i|w_{t-1},w_{t-2},w_{t-3})$</p>
<h3>Embeddings</h3>
<p>Embedding 过程
<img src="/img/nlp8_9.png" alt=""></p>
<p>经常是不会单独训练embeddings的，而是伴随训练网络的过程。</p>
<p>现在展示一个结构允许embeddings可以被学习。为了实现这个功能，增加一层。</p>
<p><img src="/img/nlp8_10.png" alt=""></p>
<ul>
<li>Select three embeddings from E:</li>
<li>Multiply by W:</li>
<li>Multiply by U:</li>
<li>Apply softmax: After the sofrmax,each node i in the output layer estimates the probability $P(w_{t}=i|w_{t-1},w_{t-2},w_{t-3})$</li>
</ul>
<p>如果我们使用e代表投影层，一个神经网络语言模型如下：</p>
<p>$$e=(Ex_{1},Ex_{2},...Ex)$$
$$h=\sigma (Wx+b)$$
$$z= Uh$$
$$y=softmax(z)$$</p>
<h3>Training the neural language model</h3>
<p>参数 $\theta =E,W,U,b$</p>
<p>对于一个句子开始训练，在每一个单词，计算交叉熵(negative log likelihood)损失是:</p>
<p>$$L=-logp(w_{t}|w_{t-1},...,w_{t-n+1})$$</p>
<p>梯度计算:</p>
<p>$$\theta_{t+1}=\theta_{t}-\eta \frac{\partial logp(w_{t}|w_{t-1},...,w_{t-n+1})}{\partial \theta }$$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/30/Localization-Overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/30/Localization-Overview/" itemprop="url">Localization Overview</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-30T18:57:07+08:00">
                2018-01-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Total Probability</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/24/R-Basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/24/R-Basics/" itemprop="url">R Basics</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-24T19:13:24+08:00">
                2018-01-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>R Basics</h2>
<p>文本分析包； TextCat</p>
<p>图形包: ggplot2</p>
<pre><code>install.packages('ggplot2')
library(ggplot2)

install.packages('RColorBrewer')
library(RColorBrewer)

data(diamonds)

# create scatterplot of price vs .carat color 
qplot(data=diamonds,x=carat,y=price,color=cut)+scalar_color_brewer(palette='Accent')
</code></pre>
<p><img src="/img/dv_01.png" alt=""></p>
<p><a href="https://www.statmethods.net/" target="_blank" rel="noopener">quick R</a>
<a href="http://www.cookbook-r.com/" target="_blank" rel="noopener">R Cookbook</a></p>
<p>getwd()获取工作目录</p>
<p>读取csv: read.csv(&quot; &quot;)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/24/Course-Information/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/24/Course-Information/" itemprop="url">Course Information</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-24T19:03:18+08:00">
                2018-01-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Course Information</h2>
<p>学习资源</p>
<ul>
<li><a href="http://swirlstats.com/students.html" target="_blank" rel="noopener">swirl</a>:Interactive mini-courses in R (great for beginners)</li>
<li><a href="https://www.rdocumentation.org/packages/DescTools/versions/0.99.19" target="_blank" rel="noopener">Listing of commands for R</a></li>
<li><a href="https://www.datacamp.com/community/tutorials/r-data-import-tutorial" target="_blank" rel="noopener">Data import tutorial</a></li>
<li><a href="https://www.statmethods.net/management/reshape.html" target="_blank" rel="noopener">Reshaping data</a></li>
<li>robustHD:
<ul>
<li><a href="http://www.nicebread.de/installation-of-wrs-package-wilcox-robust-statistics/" target="_blank" rel="noopener">Loading robustHD</a></li>
<li><a href="https://cran.r-project.org/web/packages/robustHD/robustHD.pdf" target="_blank" rel="noopener">Using robustHD</a></li>
<li><a href="https://www.rdocumentation.org/packages/robustHD/versions/0.5.1" target="_blank" rel="noopener">Reference for robustHD</a></li>
</ul>
</li>
</ul>
<p>依赖环境</p>
<p>R Datasets: iris,faithful,mtcars</p>
<p>To loda iris flower dataset(iris),</p>
<pre><code>data(iris)
head(iris)
levels(iris$Species)
data(faithful)
data(mtcars)
</code></pre>
<p>Libraries</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/23/7-Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/23/7-Logistic-Regression/" itemprop="url">7 Logistic Regression</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-23T20:26:08+08:00">
                2018-01-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Logistic Regression</h2>
<p>介绍第二个分类算法叫做多多项逻辑回归，一些涉及到语言处理的叫做最大熵模型。贝叶斯分类器和逻辑回归最大的不同之处是逻辑回归是一个discriminative(判别式)模型，而贝叶斯是一个generative(生成式)模型。</p>
<pre><code>根据贝叶斯公式，选择类别y是一个间接的过程，因此是一个生成式模型: 模型被训练从类别y中产生数据x。P(x|y)给定y，选择x。
而判别式模型一般是直接计算P(y|x)
</code></pre>
<p>$$\hat{y}=\underset{y}{argmax}P(y|x)$$</p>
<p>根据推导过程，并结合线性函数和归一化过程。</p>
<p>$$P(c|x) = \frac{exp(\sum_{i=1}{w_{i}f_{i}(c,x)})}{\sum_{c' \in C} (\sum_{i=1} w_{i}f_{i}(c',x))}$$</p>
<pre><code>其中fi是一个实值函数，在语言处理中一般使用二指特征，对每一个特征都有一个权重值，但是特征值是0还是1需要根据具体的输入数据进行判定。
</code></pre>
<p>逻辑回归就是在线性回归的基础上应用logit函数，然后得出的结果就是P(y=true|x)的概率</p>
<h2>Features in Multinomial Logistic Regression</h2>
<p>假定文本分类问题，需要知道是否将情绪标记为class +，-；
<img src="/img/nlp6_8.png" alt=""></p>
<pre><code>注意在lr中，一般定义特征函数为二元值，0，1分布。
</code></pre>
<h2>Classification in Multinomial Logistic Regression</h2>
<p><img src="/img/nlp6_9.png" alt="">
根据上述图片的描述，P(+|x)和P(-|x)可以被Eq.8.11 计算</p>
<p><img src="/img/nlp6_10.png" alt=""></p>
<p>如果文本的目标是做分类，那就不需要用上面的公式，直接使用下面的公式，也就是之前的贝叶斯公式即可。
<img src="/img/nlp6_11.png" alt=""></p>
<p>上面公式的索引从0-N,是查看所有的特征，但是实际上只需要看非零特征就可以了。</p>
<h2>Learning Logistic Regression</h2>
<p>LR训练方式是通过条件最大似然估计方式。选择参数w最大化y的概率，在给定数据x的情况下。</p>
<p>$$\hat{w}=\underset{c \in C}{argmax}logP(y^{j}|x^{j})$$</p>
<p>对于整个训练集就是上面右半部分的求和过程</p>
<p>$$\hat{w}=\underset{c \in C}{argmax}\sum _{j}logP(y^{j}|x^{j})$$</p>
<p><img src="/img/nlp6_12.png" alt=""></p>
<pre><code>上面的公式:
j代表每一个句子
i代表每一个特征，fi()代表每一个特征的特征函数，
j=1
	求和: i&lt;1~N; wi*fi(yj|xj) ，这里的yj是已知的。
	分母； 归一化过程，先令 y'=class1; 也就是每一个yj都需要
	将所有的class 遍历一遍。
</code></pre>
<p>求解权重过程变成了一个优化函数</p>
<p><img src="/img/nlp6_13.png" alt=""></p>
<p>左边就是数特征fk为1的次数(观察次数)，右边是期望的次数。</p>
<p>$$L'(w)=\sum_{j} Observed \ count(f_{k}) - Expected\  scount(f_{k})$$</p>
<h2>Regularization</h2>
<p>如果一个特征完美预测了结果只是因为它仅仅出现在一个类中，将会被设定一个非常高的权重。这种问题叫做overfitting 过拟合。</p>
<p>避免overfitting一个正则化的方法是增加一个函数。</p>
<p>$$\hat{w}=\underset{w}{argmax} \sum_{j} logP(y^{j}|x^{j})-\alpha R(w)$$</p>
<p>R(w)，正则化项，为了惩罚大的权重。</p>
<p>因此如果权重完美的匹配了训练数据，但是用很多有较大值的权重，将会被惩罚，相比于那些匹配训练数据了，但是效果不是最好的，但是使用了较小值的权重而言。</p>
<p>L2 regularization 是权重的二次方，称为欧几里德距离。</p>
<p>$$R(W) = ||W||^2_{2} = \sum_{j=1}^n w^2_{j}$$</p>
<p>L1 regularization 是一个线性函数，曼哈顿距离。</p>
<p>$$R(W) = ||W||<em>{1} = \sum</em>{i=1}^n |w_{i}|$$</p>
<p>这些正则化的类型来自于统计学，L1正则化叫做lasso 回归，L2正则化叫做ridge regression。两者都经常用在语言处理追踪。L2正则化更加容易去优化，简单的求导(2w),l1正则化更加复杂点。但是L2权重向量更加偏向于小向量，l1权重向量偏向于大向量，但是许多值都是0。 因此，L1正则化会生成稀疏箱量矩阵，也就是更少的特征。</p>
<p>下面用贝叶斯公式来解释L2约束。</p>
<p>L1和L2都会有贝叶斯解释来约束权重的先验值。L1正则化可以看作一个拉普拉斯先验，L2正则化权重分布符合高斯分母当均值$\mu =0$。 高斯分布，离mean越远，概率越低。</p>
<p><img src="/img/nlp6_14.png" alt=""></p>
<p>如果将每一个权重的高斯先验，就是最大化下面的约束条件</p>
<p><img src="/img/nlp6_15.png" alt=""></p>
<h2>Feature Selection</h2>
<p>特征选择的基础是给每一个特征一个较好的度量值，排序，保留最好的一个特征。特征数目作为一个元参数可以在dev set中训练。</p>
<p>特征排序是基于它们对分类决策提供了多少信息，最常用的方法是信息增益。信息增益是告诉我们这个单词表现多少位的信息帮助我们猜测类别，可以用如下的方式计算。</p>
<p>$$G(w)=-\sum^C_{i=1}P(c_{i})logP(c_{i})+P(w)\sum^C_{i=1}P(c_{i}|w)logP(c_{i}|w)+P(\bar{w})\sum^C_{i=1}P(c_{i}|\bar{w})log(P(c_{i}|\bar{w}))$$</p>
<pre><code>（1）原始数据 先按照类别c进行分类，计算熵
 遍历c
	计算条件概率的熵P(ci|w)logP(ci|w),w是文档中出现的单词
	计算不是该文档中该单词的熵。
</code></pre>
<p>特征选择对于没有正则化的类来说是非常重要的。</p>
<h2>Choosing a classifier and features</h2>
<p>NB分类器在小数据集和短文档中表现的更好，但是lr和svm在大数据集中表现的更好。</p>
<h2>Summary</h2>
<p>本章介绍逻辑回归(最大熵)模型。</p>
<ul>
<li>Multinomial logistic regression(also called MaxEnt or the Maximum Entropy classifier in language processing) is a discriminative model that assigns a class to an observation by computing a probability from an exponential function of a weighted set of feature of the observation</li>
<li>Regularization is important in MaxEnt models for avoiding overfitting</li>
<li>Feature selection can be helpful in removing useless features to speed up training, and is also important in unregularized for avoiding overfitting.</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/20/HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/20/HMM/" itemprop="url">HMM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-20T12:13:22+08:00">
                2018-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>HMM</h2>
<h3>1.1 马尔科夫过程</h3>
<p>随机过程最早是统计物理学的数学方法，起源于对统计力学的研究。一个简单的随机过程可以用一个状态图来描述:
<img src="/img/nlp16.png" alt=""></p>
<p>以上状态转移图用3个要素来描述</p>
<ul>
<li>状态：指系统中可能出现或存在的状态，也就是图中包含字母的圆圈，随机过程中，表示了系统随机变量的最小数目: a,e,h,i,p,t</li>
<li>转移：是指当发生只i的那个事件并满足指定条件时，系统由一种状态转移到另一种状态，也就是图中带有数字的箭头。状态转移表示两个状态之间的转换关系，在随机过程中，是指某个状态转移到下一个状态的概率</li>
<li>约束：从一个状态发出的所有箭头的概率总和为1</li>
</ul>
<p>整个状态转移过程可以用转移矩阵来表示。
<img src="/img/nlp17.png" alt="">
T表示当前状态，T+1表示下一时间状态。</p>
<h3>马尔可夫链及其概念</h3>
<p>马尔科夫过程是随机过程的一种，该过程具有如下特性：在已知系统当前状态的条件下，未来的演变与依赖过去的演变。也就是说，一个马尔科夫过程可以表示为系统在状态转移过程中，第T+1次结果只受第T次结果的影响，即只与当前状态有关，而与过去状态，即与系统的初始状态和此次转移前的所有状态无关。</p>
<p>下面给出形式化的定义。</p>
<p>设有随机过程<img src="/img/nlp18.png" alt="">,对于任意的整数<img src="/img/nlp19.png" alt="">，条件概率满足:
<img src="/img/nlp20.png" alt=""></p>
<p>称为马尔可夫链，简称马氏链。</p>
<p>马氏链的一步转移概率可以定义为:
<img src="/img/nlp21.png" alt=""></p>
<h3>应用举例</h3>
<p>任意相继两天中，雨天转晴天概率1/3,晴天转雨天1/2,0表示晴天状态，1表示雨天状态，一只5月1日为晴天，问5月3日为晴天，5月5日为雨天的概率
<img src="/img/nlp22.png" alt="">
则5月1日为晴天，5月3日为晴天的概率为5/12
继续求转移矩阵的4次方，即可得出雨天的概率为0.5995。</p>
<p>0时刻为已知状态，确定1 时刻的初始状态，确定一步转移矩阵，求2时刻的概率可以用 转移矩阵的平方。
<img src="/img/nlp23.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">JackNiu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">Tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JackNiu</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
