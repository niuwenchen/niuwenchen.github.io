<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="SpeechAndLanguageProcessing">
<meta property="og:url" content="http://niuwenchen.github.io/page/2/index.html">
<meta property="og:site_name" content="SpeechAndLanguageProcessing">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SpeechAndLanguageProcessing">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://niuwenchen.github.io/page/2/"/>





  <title>SpeechAndLanguageProcessing</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SpeechAndLanguageProcessing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">translate and learning language model</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/03/08/Spark-Deploy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/08/Spark-Deploy/" itemprop="url">Spark Deploy</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-08T13:35:04+08:00">
                2018-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Cluster Mode Overview</h2>
<p>Read through the <a href="http://spark.apache.org/docs/2.3.0/submitting-applications.html" target="_blank" rel="noopener">application submission guide</a> to learn about launching applications on a cluster</p>
<p><strong>Components</strong></p>
<p>Spark 应用在集群上作为独立的进程组来运行，在您的 main 程序中通过 SparkContext 来协调（称之为 driver 程序）。</p>
<p>具体的说，为了运行在集群上，SparkContext 可以连接至几种类型的 Cluster Manager（既可以用 Spark 自己的 Standlone Cluster Manager，或者 Mesos，也可以使用 YARN），它们会分配应用的资源。一旦连接上，Spark 获得集群中节点上的 Executor，这些进程可以运行计算并且为您的应用存储数据。接下来，它将发送您的应用代码（通过 JAR 或者 Python 文件定义传递给 SparkContext）至 Executor。最终，SparkContext 将发送 Task 到 Executor 以运行。</p>
<p><img src="http://spark.apache.org/docs/2.3.0/img/cluster-overview.png" alt=""></p>
<p>这里有几个需要注意的地方</p>
<ol>
<li>每个application获取到executor进程，会保持在应用的生命周期中并期望在多个线程中运行Task。在调度方面和Executor方面隔离。</li>
<li>Spark不知道底层的Cluster Manager到底是什么类型的。 只要获得Executor进程，并且彼此之间通信</li>
</ol>
<p><strong>Cluster Manager类型</strong></p>
<ul>
<li>Standalone: Spark中使得更容易来安装集群的一个简单的Cluster Manager</li>
<li>Mesos: 一个通用的Cluster Manager，可以运行Hadoop Mapredce和其他服务应用</li>
<li>Hadoop YARN: Hadoop 2的ResourceManager</li>
<li>Kubernetes – an open-source system for automating deployment, scaling, and management of containerized applications.</li>
</ul>
<p>** 提交应用 **
使用spark-submit 脚本可以提交应用程序至任何类型的集群，在 <a href="http://spark.apache.org/docs/2.3.0/submitting-applications.html" target="_blank" rel="noopener">application submitting</a>可以获得详细信息。</p>
<p>**监控 **</p>
<p>每个 driver 程序有一个 Web UI，通常在端口 4040 上，它展示了关于运行 task，executor，和存储使用情况的信息。在网页浏览器中访问这个 UI : http://&lt;driver-node&gt;:4040。<a href="http://spark.apache.org/docs/2.3.0/monitoring.html" target="_blank" rel="noopener">监控指南</a> 也描述了其它的监控选项。</p>
<p><strong>Job调度</strong></p>
<p>Spark既可以在应用间(Cluster Manager级别)，也可以在应用内(如果一个application中发生多个计算) 调度资源。<a href="http://spark.apache.org/docs/2.3.0/job-scheduling.html" target="_blank" rel="noopener">job scheduling overview</a></p>
<h2>Spark Standalone Mode</h2>
<p>安装过程</p>
<pre><code>1 解压: tar
2 local模式: 直接使用
3 Standalone模式:
	mv spark-env.sh.template  spark-env.sh
	export JAVA_HOME=/usr/lib/jdk
	export SPARK_MASTER_IP=cd-bigdata02

4 slaves
	cd-bigdata02
	cd-bigdata03
</code></pre>
<p>启动之后:</p>
<p><img src="/img/spark0.png" alt=""></p>
<pre><code>cores是worker的逻辑cpu个数
cat /proc/cpuinfo |grep &quot;processor&quot;|wc -l
分别是4 和 8
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/03/02/scrapy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/02/scrapy/" itemprop="url">scrapy</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-02T10:01:50+08:00">
                2018-03-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Scrapy</h2>
<p>测试网址
<a href="http://quotes.toscrape.com/page/1/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/</a></p>
<p>def start_requests(self):</p>
<pre><code>must return an iterable of Requests(you can return a 
list of requests of write a generator function) which 
Spider will  begin to crawl from. Subsequent requests 
will be generated successively form these initial 
requests
</code></pre>
<p>parse()</p>
<pre><code>The response parameter is an instance of TextResponse 
that holds the page content and has further helpful 
methods to handle it 
</code></pre>
<p>The parse() method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (Request) from them.</p>
<p><strong>How to run our spider</strong></p>
<pre><code>scrapy crawl quotes

&gt;&gt;&gt; response.css('title')
[&lt;Selector xpath='descendant-or-self::title' data='&lt;title&gt;Quotes to Scrape&lt;/title&gt;'&gt;]

response.css('title') is a list-like called SelectorList, which represents a list of selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the seletion or extract the data.
</code></pre>
<h2>Using Selectors</h2>
<pre><code>https://doc.scrapy.org/en/latest/_static/selectors-sample1.html
&gt;&gt;&gt; response.xpath('//title/text()')
[&lt;Selector (text) xpath=//title/text()&gt;]
&gt;&gt;&gt; response.css('title::text')
[&lt;Selector (text) xpath=//title/text()&gt;]

response.css('img')

[&lt;Selector xpath='descendant-or-self::img' data='&lt;img src=&quot;image1_thumb.jpg&quot;&gt;'&gt;,
&lt;Selector xpath='descendant-or-self::img' data='&lt;img src=&quot;image2_thumb.jpg&quot;&gt;'&gt;,
&lt;Selector xpath='descendant-or-self::img' data='&lt;img src=&quot;image3_thumb.jpg&quot;&gt;'&gt;,
&lt;Selector xpath='descendant-or-self::img' data='&lt;img src=&quot;image4_thumb.jpg&quot;&gt;'&gt;,
&lt;Selector xpath='descendant-or-self::img' data='&lt;img src=&quot;image5_thumb.jpg&quot;&gt;'&gt;]


&gt;&gt;&gt; response.css('img').xpath('@src').extract()
[u'image1_thumb.jpg',
u'image2_thumb.jpg',
u'image3_thumb.jpg',
u'image4_thumb.jpg',
u'image5_thumb.jpg']


说明xpath中里面的某一个属性用 @xxx 来表示



&gt;&gt;&gt; response.xpath('//base/@href').extract()
[u'http://example.com/']

&gt;&gt;&gt; response.css('base::attr(href)').extract()
[u'http://example.com/']

&gt;&gt;&gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/@href').extract()
[u'image1.html',
u'image2.html',
u'image3.html',
u'image4.html',
u'image5.html']

&gt;&gt;&gt; response.css('a[href*=image]::attr(href)').extract()
[u'image1.html',
u'image2.html',
u'image3.html',
u'image4.html',
u'image5.html']

&gt;&gt;&gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/img/@src').extract()
[u'image1_thumb.jpg',
u'image2_thumb.jpg',
u'image3_thumb.jpg',
u'image4_thumb.jpg',
u'image5_thumb.jpg']

&gt;&gt;&gt; response.css('a[href*=image] img::attr(src)').extract()
[u'image1_thumb.jpg',
u'image2_thumb.jpg',
u'image3_thumb.jpg',
u'image4_thumb.jpg',
u'image5_thumb.jpg']
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/03/01/hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/01/hadoop/" itemprop="url">hadoop</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-01T13:39:05+08:00">
                2018-03-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>hadoop的集群是基于master/slave模式，namenode和jobtracker属于master，datanode和tasktracker属于slave，master只有一个，而slave有多个.</p>
<p>SecondaryNameNode内存需求和NameNode在一个数量级上，所以通常secondary NameNode（运行在单独的物理机器上）和 NameNode 运行在不同的机器上。</p>
<p>JobTracker对应于NameNode,TaskTracker对应于DataNode.</p>
<p>DataNode和NameNode是针对数据存放来而言的.JobTracker和TaskTracker是对于MapReduce执行而言的.</p>
<p>mapreduce中几个主要概念，mapreduce 整体上可以分为这么几条执行线索：</p>
<p>jobclient，JobTracker与TaskTracker。</p>
<p>1、JobClient会在用户端通过JobClient类将已经配置参数打包成jar文件的应用存储到hdfs，并把路径提交到Jobtracker,然后由JobTracker创建每一个Task（即 MapTask 和 ReduceTask） 并将它们分发到各个TaskTracker服务中去执行。</p>
<p>2、JobTracker是一master服务，软件启动之后JobTracker接收Job，负责调度Job的每一个子任务。task运行于TaskTracker上，并监控它们，如果发现有失败的task就重新运行它。一般情况应该把JobTracker 部署在单独的机器上。</p>
<p>3、TaskTracker是运行在多个节点上的slaver服务。TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务。 TaskTracker 都需要运行在HDFS的DataNode上。</p>
<p>secondaryNameNode: 他的目的使帮助 NameNode 合并编辑日志，减少 NameNode 启动时间</p>
<p>列决几个Hadoop生态圈的组件并作简要概述
Zookeeper:是一个开源的分布式应用程序协调服务,基于zookeeper可以实现同步服务，配置维护，命名服务。</p>
<p>Flume:一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。</p>
<p>Hbase:是一个分布式的、面向列的开源数据库, 利用Hadoop HDFS作为其存储系统.</p>
<p>Hive:基于Hadoop的一个数据仓库工具，可以将结构化的数据档映射为一张数据库表，并提供简单的sql 查询功能，可以将sql语句转换为MapReduce任务进行运行。</p>
<p>Sqoop:将一个关系型数据库中的数据导进到Hadoop的 HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
<p>4 正常工作的Hadoop集群中Hadoop都分别需要启动哪些进程，它们的作用分别是什么?</p>
<p>a) NameNode 是hadoop中的主服务器，管理文件系统名称空间和对集群中存储的文件的访问，保存有metadate</p>
<p>b) SecondaryNameNode 不是NameNode的冗余守护进程，而是提供周期检查点检查点和清理服务。帮助NN合并edits log,减少NN启动时间</p>
<p>c) DataNode 负责管理连接到节点的存储(一个集群中可以有多个节点)。 每个存储数据的节点运行一个datanode守护进程。</p>
<p>d) ResourceManager(JobTracker),负责调度DataNode上的工作。每个DataNode有一个TaskTracker，执行实际工作。</p>
<p>e) NodeManager (TaskTracker)执行任务</p>
<p>4 HDFS 的存储机制是什么</p>
<p>客户端通过把请求发送给NameNode active，NN会把文件切成1到N个固定大小的block(一般默认为128M)并上传到DataNode中。当所有block拷贝结束时，NN会立即通知客户端上传结果。但此时上传的流程还未结束。DN还需要根据配置信息的副本数量，在不同的机架节点上通过局域网做数据拷贝工作。</p>
<p>5 Hadoop中的job和task之间的区别是什么</p>
<p>JobTracker是一个master服务，软件启动之后，JobTracker接受Job，负责调度Job的每一个子任务task运行于TaskTracker上，并监控他们，如果发现有失败的task就重新运行它。一般情况应该把JobTracker部署在单独的机器上。</p>
<p>TaskTracker是运行在多个节点上的slaver服务。TaskTracker主动与JobTracker通信，接收作业，并负责执行每一个任务</p>
<p>7 如何决定一个job的map和reduce的数量</p>
<pre><code>splitSize = max{minSize,min{maxSize,blockSize}}
map的数量由处理的初级分成的block熟练决定，= total_size/split_size
reduce的数量job.setNumReduceTasks(x);x 为reduce的数量
</code></pre>
<p>8 如何为一个Hadoop任务设置mappers的数量</p>
<p>InputFormat在默认情况下会根据hadoop集群HDFS块大小进行分片，每一个分片会由一个map任务来进行处理，当然用户还是可以通过参数mapred.min.split.size 参数在作业提交客户端进行自定义设置。还有一个重要参数就是mapred.map.tasks, 这个参数设置的map数量仅仅是一个提示，只有当InputFormat决定了map任务的个数比mapred.map.tasks值小时才起作用。同样，Map任务的个数也能通过使用JobConf的conf.setNumMapTasks()来手动的设置。这个方法能够用来增加map任务的个数，但是不鞥设定任务的个数小于Hadoop系统通过分隔输入数据得到的值。</p>
<p>9 如何为一个Hadoop任务设置要创建的reducer的数量</p>
<p>纯粹的mapreduce task的reduce task数很简单，就是参数mapred.reduce.tasks 的值</p>
<p>10 MapReduce 中排序发生的几个阶段</p>
<p>4个阶段</p>
<p>Splitting: 在进行map计算之前，mapreduce会根据输入文件计算输入分片(inputsplit),每个输入分片针对一个map任务。输入分片存储的并非数据本身，</p>
<p>11 请描述Mapreduce中shuffle阶段的工作流程，如何优化shuffle阶段</p>
<p>分区，排序，溢写，拷贝到对应reduce机器上，增加conbiner，压缩溢写的文件。</p>
<p>12 Mapreduce中combiner的作用是什么，一般使用情景，哪些情况不需要？</p>
<p>MR作业中的Map阶段会输出结果数据到磁盘中</p>
<p>Combiner只适应于那种reduce的输入类型完全一直，且不影响最终结果的场景，比如累加，最大值等。也可以用于过滤数据，在map端将无效的数据过滤到</p>
<p>combiner的作用就是在map端对输出先做一次合并，以减少传输到reducer的数据量</p>
<p>combiner最基本是实现本地key的归并，具有类似本地reduce，那么所有的结果都是reduce</p>
<p>使用combiner，先完成的map会在本地聚合，提升速度。</p>
<pre><code>map--&gt; 本地reduce --&gt;集群reduce
</code></pre>
<p>13  如果没有定义partitioner，那数据在被送达reducer前是如何被分区的</p>
<pre><code>如果没有自定义的partitioning，则默认的partition算法，即根据每一条数据的key的hascode 值 取模reduce的数量，得到的数字就是reducer 分区号
</code></pre>
<p>14 Map阶段结束后，Hadoop框架会处理：Partitioning，shuffle和sort，</p>
<pre><code>map task上的shuffle结束，此时reduce task 上的shuffle开始，抓取fetch所属于自己分区的数据，同时将这些分区的数据进行排序sort(默认的排序是根据每一条数据的键的字典排序)，进而将数据进行合并merge，即根据key相同的，将其value组成一个集合，最后输出结果。
</code></pre>
<p>15 一个mr作业跑得比较慢，如何来优化，</p>
<pre><code>数据倾斜
	数据倾斜可能是partition不合理，导致部分partition中的数据过多，部分过少，通过分析数据，自定义分区解决。

合理设置map和reduce数: 两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致map，reduce任务间竞争资源，造成处理超时等错误。

设置map，reduce共存: 调整slowstart.completedmaps参数，是map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。

合并小文件: 在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致mr运行较慢

减少spill次数
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/03/01/mysql-基础/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/01/mysql-基础/" itemprop="url">mysql 基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-01T09:32:45+08:00">
                2018-03-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/26/CKY/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/26/CKY/" itemprop="url">CKY</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-26T16:13:38+08:00">
                2018-02-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>CKY(Cocke-Kasami-Younger) and Earley Parsing Algorithms</h2>
<p><strong>Context Free Grammar</strong></p>
<p>A Context-free Grammar(CFG) is a 4-tuple:
$$G={N,\sum  ,P,S}$$</p>
<p><img src="/img/nlp_cky1.png" alt=""></p>
<p><strong>Chomsky Normal Form 乔姆斯基范式</strong></p>
<p>A context-free grammar</p>
<pre><code>A--&gt; \alpha
A--&gt; BC
Any CFG can be converted a weakly equivalent grammar in CNF
</code></pre>
<h2>Parsing Algorithm</h2>
<pre><code>CFGs are basis for describing(syntactic) structure of NL sentences
Thus- Parsing Algorithms are core of NL analysis systems
Recognition vs. Parsing:
	Recognition-deciding the membership in the language
	Parsing- Recogintion+producing a parse tree for it
Parsing is more difficult than recogintion(time complexity)
Ambiguity - an input may have exponentially many parses.
</code></pre>
<h2>Parsing Algorithms</h2>
<pre><code>Top-down vs bottom-up
Top-down:(goal-driven): from the start symbol down
Bottom-up:(data-driven): from the symbols up.

Naive vs. dynamic programming
Naive: enumerate everything
backtracking: try something, discard partial solutions.
Dynamic programming: save partial solutions in a table

Example:
CKY: bottom-up dynamic programming
Earley parsing: top-down dynamic programming.
</code></pre>
<h2>CKY</h2>
<pre><code>one of the earliest recogintion and parsing algorithms 
The standard version of CKY can only recoginze languages defined by context-free grammars in Chomsky Normal Form(CNF)
It is also possible to extend the CKY algorithm to handle some grammars which are not in CNF
	Harder to understand
Based on a &quot;dynamic programming&quot; approach:
	Build solutions compositionally from sub-solutions
Uses the grammar directly
</code></pre>
<p>Considers every possible consective(连续的，连贯的) subsequence of letters and sets $k \in T[i,j]$, if the sequence of letters starting frm i to j can be generated from the non-terminal K.</p>
<p>Once it has considered sequences of length 1 , it goes on to sequences of length 2,and so on</p>
<p>For subsequences of length 2 and greater, it considers every possible partition of the subsequence into two halves, and checks to see if there is some production A-&gt;BC sunch that B matches the first half and C matches the second half. If so, it records A as matching the whole subsequence.</p>
<p>Once this process if completed, that sentence if recognnized by the grammar if the entire string is matched by the start symbol.</p>
<p>**Observation: ** any portion of the input string spanning i to j can be split at k, and structure can then be built using sub-solutions spanning i to k and sub-solitions spanning k to j.</p>
<p>**Meaning: ** Solution to problem [i,j] can constructed from solution to sub problem[i,k] and solution to sub problem [k,j]</p>
<p><img src="/img/nlp_cky2.png" alt=""></p>
<p><img src="/img/nlp_cky3.png" alt=""></p>
<p>The algorithm is &quot;bottom-up&quot; in that we start with bottom of derivation tree.</p>
<p><img src="/img/nlp_cky4.png" alt=""></p>
<h2>The CKY Algorithm</h2>
<pre><code>function CKY(word w,grammar P) returns table
for i&lt;- from 1 to length(w) do
	table[i-1,i] &lt;- {A| A-&gt;wi in P}
for j &lt;- from 2 to length(w) do
	for i &lt;- form j-2 down to 0 do
		for k &lt;- i+1 to j-1 do
			table[i,j] &lt;- table[i,j] 并 {A|A-&gt; BC in P,B in table[i,k],C in table[k,j]}
</code></pre>
<p><img src="/img/nlp_cky5.png" alt=""></p>
<p>这种解析过程能够判断出句子的形式</p>
<h2>Parsing results</h2>
<pre><code>We keep the results for every wij in a table
Note that we only need to fill in entries up to the diagonal
Every entry in the table T[i,j] can contains up to r=|N| symbols (the size of non-terminal set)
We can use lists or a Boolean n*n*r table
We then want to find T[0,b,S] = true
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/26/12-Syntactic-Parsing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/26/12-Syntactic-Parsing/" itemprop="url">12 Syntactic Parsing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-26T13:09:36+08:00">
                2018-02-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Syntactic Parsing(句法分析)</h2>
<p>Syntactic Parsing is the task of recognizing a sentence and assigning a syntactic structure to it. This chapter focuses on the kind of structures assigned by context-free grammers of the kind described in chapter 11. We need to specify algorithms that employ these grammers to efficiently produce correct trees.</p>
<p>Parse trees are directly useful in applications such as grammar checking in word-processing systems: a sentence that cannot be paesed may have grammatical errors(or at least be hard to read). More typically, paese trees serve as an important intermediate state of representation for semantic analysis(as we show in chapter 20) and thus play an important role in applications like question answering and information extration. For example, to answer the question</p>
<pre><code>What books were written by British women authors before 1800?
</code></pre>
<p>we’ll need to know that the subject of the sentence was what books and that the byadjunct was British women authors to help us figure out that the user wants a list of books (and not a list of authors).</p>
<h2>1 Ambiguity(含糊)</h2>
<p>structural ambiguity,which arises from many commonly used rules in phrase-structure grammers.
<img src="/img/nlp12_01.png" alt=""></p>
<p>Structural ambiguity,appropriately enough,comes in many forms. Two common kinds of ambiguity are attachment ambiguity(附着歧义) and coordination ambiguity(并列歧义).</p>
<p>A sentence has an attahcment ambiguity if a particular constituent can be attached to the parse tree at more than one place.</p>
<p>In coordination ambiguity different sets of phrase can be conjoined by a conjunction like and. the phrase old men and women [old [men and women]] or as [old men ]and women;</p>
<p>These ambigutions combine in complex ways in real sentences. A program that summarized the news, for example, would need to be able to parse sentences like the following from the Brown corpus</p>
<pre><code>Persident Kennedy today pushed aside other White House business to devote all his time and attention to working on the Berlin crisis address he will deliver tomorrow night to the American people over nationwide television and radio.
</code></pre>
<p>The fact that there are many grammatically correct byt semantically unreasonable parses for naturally occurring sentences is an irksome problem that affects all parsers. Ultimately,most natural language processing systems need to be able to choose a single correct parse from the multitude of possible parses through a process of syntactic disambiguation. Effiective disambiguation algorithms require statistical,semantic,and contextual knowledge sources that vary in how well they can be integrated into parsing algorithms.</p>
<p>CKY algorothm presented in the next section is designed to efficiently handle structural ambiguities of the kind we've been discussing. And as we'll see in Chapter 13, there are straightforward ways to integrate statistical techniques into the basic CKY framework to produce highly accurate parsers.</p>
<h2>CKY Parsing: A Dynamic Programming Approach</h2>
<p>The dynamic programming advantage arises from the context-free nature of our grammer rules - once a constituent has been discovered in a segment of the input we can record its presence and make it available for use in any subsequent derivation that might require it. This provides both time and storage efficiencies since subtrees can be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-Younger(CKY) algorithm,the most widely used dynamic-programming based approach to parsing.</p>
<h3>12.2.1 Conversion to Chomsky Normal Form</h3>
<h3>12.2.2 CKY Recognition</h3>
<p><img src="/img/nlp12_02.png" alt=""></p>
<p><img src="/img/nlp12_03.png" alt=""></p>
<p><img src="/img/nlp12_04.png" alt=""></p>
<h3>12.2.3 CKY Parsing</h3>
<p>上面的是recoginzer过程不是parser过程。</p>
<h3>12.2.4 CKY in Practice</h3>
<p>Obviously,as things stand now, our parser isn't returning trees that are consistent with the grammar given to us by our friendly syntacticians. In addition to making our grammar developers unhappy. the conversion to CNF will complicate any syntax-driven approach to semantic analysis.</p>
<h2>12.3 Partial Parsing</h2>
<p>An alternative sstyle of partial parsing is known as chunking(组块分析). bracket notation(括号标记)</p>
<pre><code>[NP The morning flight][pp from][NP Denver][VP has arrived.]
</code></pre>
<p>non-overlapping(无重叠的)</p>
<p>Note that in this example all the words are contained in some chunk. This will not be the case in all chunking applications. Many words in any input will often fall outside of any chunk, for example,in systems searching for base NPs in their inputs,as in the following:</p>
<pre><code>[NP The morning flight] from [NP Denver] has arrived
</code></pre>
<h3>12.3.1 Machine Learning-Based Approaches to Chunking</h3>
<p>State-of-the-art(最先进的) to chunking use supervised machine learning to train a chunker by using annotated data as a training set.We can view this task as one of sequence labeling, where a classifier is trained to label each element of the input sequence. Any of the standard approaches to training classifiers apply to this problem.</p>
<p>The first step in such an approach is to cast the chunking process in a way that is amenable to sequence labeling.A partiaularly fruitful approach has been to treat chunking as a tagging task similar to part-of-speech tagging. In this approach,a small tagset simultaneously encodes both the segmentation and the labeling of the chunks in the input. The standard way to do this is called IOB tagging and is accomplished by introducing tags to represent the beginning(B) and internal(I) parts of each chunk, as well as those elements of the input that are outside any chunk.Under this scheme,the size of tagset is (2n+1), where n is the number of categories to be classified. The following example shows the bracketing notation of on page 206 reframed as a tagging task:</p>
<pre><code>The morning flight from Denver has arrived
B_NP I_NP   I_NP   B_PP B_NP  B_VP I_VP

The same sentence with only the base-NPs tagged illustrates the role of the O tags
The morning flight from Denver has arrived
B_NP  I_VP  I_Np    O   B_NP   O   O
</code></pre>
<p>Notice that there is no explicit encoding of the end of a chunk in this scheme; the end of any chunk is implicit in any transition from an I or B to a B or O tag. This encoding reflets the notion that when sequentially labeling words,it is generally easier(at least in English) to detect the begining of a new chunk than it is to know when a chunk has ended.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/26/RandomForest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/26/RandomForest/" itemprop="url">RandomForest</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-26T10:43:16+08:00">
                2018-02-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>随机森林</h2>
<p>随机森林非常像机器学习实践中的AdaBoost算法，但区别在于没有迭代，随机森林的树长度不限制。</p>
<p>因为没有迭代过程，不像AdaBoost那样需要迭代，不断更新每个样本机器子分类器的权重。因此模型相对简单，不容易出现过拟合。</p>
<p><img src="/img/ml01.png" alt="">
随机森林可以理解成CART树森林，是由多个CART树分类器构成的集成学习模式。其中每个Cart树可以理解为一个议员，从样本集里面随机有放回的抽取一部分进行训练，这样，多个树分类器就构成了一个训练模型，可以理解为一个议会。</p>
<p>然后将要分类的样本带入每一个树分类器，然后以少数服从多数的原则，表决出这个样本的最终分类类型。</p>
<p>设有N个样本，M个变量个数，具体流程如下；</p>
<p>1 确定一个值吗，用来表述每个树分类器选取多少个变量</p>
<p>2 从数据集中有放回的抽取k个样本集，用他们来创建k个树分类器。另外还伴随生成了k个逮外数据，用来后面做检测。</p>
<p>3 输入待分类样本之后，每个树分类器会进行分类，选举。</p>
<p>参数</p>
<pre><code>预选变量个数
随机森林中树的个数
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/26/MachineLearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/26/MachineLearning/" itemprop="url">MachineLearning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-26T09:18:23+08:00">
                2018-02-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>there are 3 types of Machine Learning Algorithms..</p>
<ol>
<li>
<p>Supervised Learning</p>
<p>How it works: This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data. Examples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc.</p>
</li>
<li>
<p>Unsupervised Learning</p>
<p>How it works: In this algorithm, we do not have any target or outcome variable to predict / estimate.  It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. Examples of Unsupervised Learning: Apriori algorithm, K-means.</p>
</li>
<li>
<p>Reinforcement Learning:</p>
<p>How it works:  Using this algorithm, the machine is trained to make specific decisions. It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning: Markov Decision Process</p>
</li>
</ol>
<p>List of Common Machine Learning Algorithms</p>
<pre><code>Linear Regression
Logistic Regression
Decision Tree
SVM
Naive Bayes
kNN
K-Means
Random Forest
Dimensionality Reduction Algorithms
Gradient Boosting algorithms
GBM
XGBoost
LightGBM
CatBoost
</code></pre>
<h2>1 Linear Regression</h2>
<p>It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation Y= a *X + b.</p>
<p>Look at the below example. Here we have identified the best fit line having linear equation y=0.2811x+13.9. Now using this equation, we can find the weight, knowing the height of a person.</p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Linear_Regression.png" alt=""></p>
<p>Multiple Linear Regression(多变量线性回归)，采用多个参数来定义模型，Polynomial Regression多项式回归</p>
<p>While finding best fit line, you can fit a polynomial or curvilinear regression. And these are known as polynomial or curvilinear regression.</p>
<p>Python Code</p>
<pre><code>#Import Library
#Import other necessary libraries like pandas, numpy...
from sklearn import linear_model
#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays
x_train=input_variables_values_training_datasets
y_train=target_variables_values_training_datasets
x_test=input_variables_values_test_datasets
# Create linear regression object
linear = linear_model.LinearRegression()
# Train the model using the training sets and check score
linear.fit(x_train, y_train)
linear.score(x_train, y_train)
#Equation coefficient and Intercept
print('Coefficient: \n', linear.coef_)
print('Intercept: \n', linear.intercept_)
#Predict Output
predicted= linear.predict(x_test)
</code></pre>
<p>R Code</p>
<pre><code>#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays
x_train &lt;- input_variables_values_training_datasets
y_train &lt;- target_variables_values_training_datasets
x_test &lt;- input_variables_values_test_datasets
x &lt;- cbind(x_train,y_train)
# Train the model using the training sets and check score
linear &lt;- lm(y_train ~ ., data = x)
summary(linear)
#Predict Output
predicted= predict(linear,x_test) 
</code></pre>
<h2>2 Logistic Regression</h2>
<p>Don’t get confused by its name! It is a classification not a regression algorithm. It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s). In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function.（它预测一个事件发生的概率通过一个logit 函数）。Hence, it is also known as logit regression. Since, it predicts the probability, its output values lies between 0 and 1 (as expected).</p>
<p>Again, let us try and understand this through a simple example.</p>
<p>Let’s say your friend gives you a puzzle to solve. There are only 2 outcome scenarios – either you solve it or you don’t. Now imagine, that you are being given wide range of puzzles / quizzes in an attempt to understand which subjects you are good at. The outcome to this study would be something like this – if you are given a trignometry based tenth grade problem, you are 70% likely to solve it. On the other hand, if it is grade fifth history question, the probability of getting an answer is only 30%. This is what Logistic Regression provides you.(解决这个问题的概率是70%.)</p>
<p>Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.</p>
<pre><code>odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk

即
logit(p) = ln(odds) 概率的logit函数值就是发生几率的对数函数值

如果按照上面的过程解读LR，那就类似线性回归； 但是如果按照feature来解读，就像是最大熵模型
</code></pre>
<p>Above, p is the probability of presence of the characteristic of interest. It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors (like in ordinary regression).</p>
<p>Python Code</p>
<pre><code>#Import Library
from sklearn.linear_model import LogisticRegression
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create logistic regression object
model = LogisticRegression()
# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)
#Equation coefficient and Intercept
print('Coefficient: \n', model.coef_)
print('Intercept: \n', model.intercept_)
#Predict Output
predicted= model.predict(x_test)
</code></pre>
<p>R Code</p>
<pre><code>x &lt;- cbind(x_train,y_train)
# Train the model using the training sets and check score
logistic &lt;- glm(y_train ~ ., data = x,family='binomial')
summary(logistic)
#Predict Output
predicted= predict(logistic,x_test)
</code></pre>
<h2>3 Decision Tree</h2>
<p>This is one of my favorite algorithm and I use it quite frequently. It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. For more details, you can read: Decision Tree Simplified.</p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/IkBzK.png" alt=""></p>
<p>In the image above, you can see that population is classified into four different groups based on multiple attributes to identify ‘if they will play or not’. To split the population into different heterogeneous groups, it uses various techniques like Gini, Information Gain, Chi-square, entropy.</p>
<p>More: <a href="https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/" target="_blank" rel="noopener">Simplified Version of Decision Tree Algorithm</a></p>
<p>Python Code</p>
<pre><code>#Import Library
#Import other necessary libraries like pandas, numpy...
from sklearn import tree
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create tree object 
model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  
# model = tree.DecisionTreeRegressor() for regression
# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)
#Predict Output
predicted= model.predict(x_test)
</code></pre>
<p>R Code</p>
<pre><code>library(rpart)
x &lt;- cbind(x_train,y_train)
# grow tree 
fit &lt;- rpart(y_train ~ ., data = x,method=&quot;class&quot;)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)
</code></pre>
<h2>4 SVM</h2>
<p>It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.</p>
<p>For example, if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as Support Vectors)(坐标)</p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM2.png" alt=""></p>
<p>Now, we will find some line that splits the data between the two differently classified groups of data. This will be the line such that the distances from the closest point in each of the two groups will be farthest away.</p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/SVM2.png" alt=""></p>
<p>In the example shown above, the line which splits the data into two differently classified groups is the black line, since the two closest points are the farthest apart from the line. This line is our classifier. Then, depending on where the testing data lands on either side of the line, that’s what class we can classify the new data as.</p>
<p>More: <a href="">Simplified Version of Support Vector Machine</a></p>
<p>Python Code</p>
<pre><code>#Import Library
from sklearn import svm
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create SVM classification object 
model = svm.svc() # there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.
# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)
#Predict Output
predicted= model.predict(x_test)
</code></pre>
<p>R Code</p>
<pre><code>library(e1071)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-svm(y_train ~ ., data = x)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)
</code></pre>
<h2>5 Native Bayes</h2>
<p>It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.</p>
<p>Naive Bayesian model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.</p>
<p>Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:</p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_rule.png" alt=""></p>
<p>Here,</p>
<pre><code>P(c|x) is the posterior probability of class (target) given predictor (attribute). 
P(c) is the prior probability of class. 
P(x|c) is the likelihood which is the probability of predictor given class. 
P(x) is the prior probability of predictor.
</code></pre>
<p>Example: Let’s understand it using an example. Below I have a training data set of weather and corresponding target variable ‘Play’. Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it.</p>
<p>Step 1: Convert the data set to frequency table</p>
<p>Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.</p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Bayes_41.png" alt=""></p>
<p>Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.</p>
<p>Problem: Players will pay if weather is sunny, is this statement is correct?</p>
<p>We can solve it using above discussed method, so P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)</p>
<p>Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64</p>
<p>Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.</p>
<p>Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.</p>
<p>Python Code</p>
<pre><code>#Import Library
from sklearn.naive_bayes import GaussianNB
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)
</code></pre>
<p>R Code</p>
<pre><code>library(e1071)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-naiveBayes(y_train ~ ., data = x)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)
</code></pre>
<h2>KNN</h2>
<p>It can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.</p>
<p>These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. First three functions are used for continuous function and fourth one (Hamming) for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing K turns out to be a challenge while performing kNN modeling.</p>
<p>Mode:<a href="">Introduction to k-nearest neighbors : Simplified</a></p>
<p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/08/KNN.png" alt=""></p>
<p>KNN can easily be mapped to our real lives. If you want to learn about a person, of whom you have no information, you might like to find out about his close friends and the circles he moves in and gain access to his/her information!</p>
<p>Things to consider before selecting kNN:</p>
<p>KNN is computationally expensive
Variables should be normalized else higher range variables can bias it
Works on pre-processing stage more before going for kNN like outlier, noise removal</p>
<p>Python Code</p>
<pre><code>#Import Library
from sklearn.neighbors import KNeighborsClassifier
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create KNeighbors classifier object model 
KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)
</code></pre>
<p>R Code</p>
<pre><code>library(knn)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;-knn(y_train ~ ., data = x,k=5)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)
</code></pre>
<h2>7 K-Means</h2>
<p>It is a type of unsupervised algorithm which  solves the clustering problem. Its procedure follows a simple and easy  way to classify a given data set through a certain number of  clusters (assume k clusters). Data points inside a cluster are homogeneous and heterogeneous to peer groups.</p>
<p>Remember figuring out shapes from ink blots? k means is somewhat similar this activity. You look at the shape and spread to decipher how many different clusters / population are present!</p>
<h2>8 Random Forest</h2>
<p>Random Forest is a trademark term for an ensemble of decision trees. In Random Forest, we’ve collection of decision trees (so known as “Forest”). To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest).</p>
<p>Each tree is planted &amp; grown as follows:</p>
<p>If the number of cases in the training set is N, then sample of N cases is taken at random but with replacement. This sample will be the training set for growing the tree.</p>
<p>If there are M input variables, a number m&lt;&lt;M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.</p>
<p>Each tree is grown to the largest extent possible. There is no pruning.</p>
<p>For more details on this algorithm, comparing with decision tree and tuning model parameters, I would suggest you to read these articles:</p>
<p><a href="https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/" target="_blank" rel="noopener">Introduction to Random forest – Simplified</a>
<a href="https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/" target="_blank" rel="noopener">Comparing a CART model to Random Forest (Part 1)</a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/" target="_blank" rel="noopener">Comparing a Random Forest to a CART model (Part 2)</a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/" target="_blank" rel="noopener">Tuning the parameters of your Random Forest model</a></p>
<p>Python</p>
<pre><code>#Import Library
from sklearn.ensemble import RandomForestClassifier
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create Random Forest object
model= RandomForestClassifier()
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)
</code></pre>
<p>R code</p>
<pre><code>library(randomForest)
x &lt;- cbind(x_train,y_train)
# Fitting model
fit &lt;- randomForest(Species ~ ., x,ntree=500)
summary(fit)
#Predict Output 
predicted= predict(fit,x_test)
</code></pre>
<h2>9 Dimensionality Reduction Algorithms（降维算法）</h2>
<p>In the last 4-5 years, there has been an exponential increase in data capturing at every possible stages. Corporates/ Government Agencies/ Research organisations are not only coming with new sources but also they are capturing data in great detail.</p>
<p>For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like or dislike, purchase history, feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.</p>
<p>As a data scientist, the data we are offered also consist of many features, this sounds good for building good robust model but there is a challenge. How’d you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others.</p>
<p>To know more about this algorithms, you can read <a href="https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/" target="_blank" rel="noopener">“Beginners Guide To Learn Dimension Reduction Techniques”</a></p>
<p>Python code</p>
<pre><code>#Import Library
from sklearn import decomposition
#Assumed you have training and test data set as train and test
# Create PCA obeject 
pca= decomposition.PCA(n_components=k) 
#default value of k =min(n_sample, n_features)
# For Factor analysis
#fa= decomposition.FactorAnalysis()
# Reduced the dimension of training dataset using PCA
train_reduced = pca.fit_transform(train)
#Reduced the dimension of test dataset
test_reduced = pca.transform(test)
#For more detail on this, please refer  this link.
</code></pre>
<p>R code</p>
<pre><code>library(stats)
pca &lt;- princomp(train, cor = TRUE)
train_reduced  &lt;- predict(pca,train)
test_reduced  &lt;- predict(pca,test)
</code></pre>
<h2>10. Gradient Boosting Algorithms</h2>
<p>10.1. GBM</p>
<p>GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.</p>
<p>More: <a href="https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/" target="_blank" rel="noopener">Know about Boosting algorithms in detail</a></p>
<p>Python Code</p>
<pre><code>#Import Library
from sklearn.ensemble import GradientBoostingClassifier
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create Gradient Boosting Classifier object
model= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)
</code></pre>
<p>R code</p>
<pre><code>library(caret)
x &lt;- cbind(x_train,y_train)
# Fitting model
fitControl &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 4, repeats = 4)
fit &lt;- train(y ~ ., data = x, method = &quot;gbm&quot;, trControl = fitControl,verbose = FALSE)
predicted= predict(fit,x_test,type= &quot;prob&quot;)[,2]
</code></pre>
<p>GradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about the <a href="">difference between these two algorithms</a>.</p>
<h3>10.2 XGBoost</h3>
<p>Another classic gradient boosting algorithm that’s known to be the decisive choice between winning and losing in some Kaggle competitions.</p>
<p>The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques.</p>
<p>The support includes various objective functions, including regression, classification and ranking.</p>
<p>One of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. This helps to reduce overfit modelling and has a massive support for a range of languages such as Scala, Java, R, Python, Julia and C++.</p>
<p>Supports distributed and widespread training on many machines that encompass GCE, AWS, Azure and Yarn clusters. XGBoost can also be integrated with Spark, Flink and other cloud dataflow systems with a built in cross validation at each iteration of the boosting process.</p>
<p>To learn more about XGBoost and parameter tuning, visit <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</a>.</p>
<p>Python Code</p>
<pre><code>from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X = dataset[:,0:10]
Y = dataset[:,10:]
seed = 1

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)

model = XGBClassifier()

model.fit(X_train, y_train)

#Make predictions for test data
y_pred = model.predict(X_test)
</code></pre>
<h3>10.3. LightGBM</h3>
<p>LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:</p>
<pre><code>Faster training speed and higher efficiency
Lower memory usage
Better accuracy
Parallel and GPU learning supported
Capable of handling large-scale data
</code></pre>
<p>he framework is a fast and high-performance gradient boosting one based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It was developed under the Distributed Machine Learning Toolkit Project of Microsoft.</p>
<p>Since the LightGBM is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.</p>
<p>Refer to the article to know more about LightGBM: <a href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/</a></p>
<p>Python Code</p>
<pre><code>data = np.random.rand(500, 10) # 500 entities, each contains 10 features
label = np.random.randint(2, size=500) # binary target

train_data = lgb.Dataset(data, label=label)
test_data = train_data.create_valid('test.svm')

param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}
param['metric'] = 'auc'

num_round = 10
bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])

bst.save_model('model.txt')

	# 7 entities, each contains 10 features
data = np.random.rand(7, 10)
ypred = bst.predict(data)
</code></pre>
<p><a href="https://wizardforcel.gitbooks.io/dm-algo-top10/content/em.html" title="数据挖掘十大算法" target="_blank" rel="noopener">https://wizardforcel.gitbooks.io/dm-algo-top10/content/em.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/09/Aho-Corasick自动机结合DoubleArrayTrie极速多模式匹配/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/09/Aho-Corasick自动机结合DoubleArrayTrie极速多模式匹配/" itemprop="url">Aho Corasick自动机结合DoubleArrayTrie极速多模式匹配</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-09T11:12:24+08:00">
                2018-02-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Aho-Corasick算法的Java实现与分析</h2>
<p>简介</p>
<pre><code>Aho-Corasick算法简称AC算法，通过将模式串预处理为确定有限状态自动机，扫描文本一遍就能结束。其时间复杂度为O(n)，即与模式串的数量和长度无关。
</code></pre>
<p>思想</p>
<pre><code>自动机按照文本字符顺序，接受字符，并发生状态转移。这些状态缓存
了&quot;按照字符转移成功(但不是模式串的结尾)&quot;、&quot;按照分i粗转移成功
(是模式串的结尾)&quot;、&quot;按照字符转移失败&quot;三种情况，因而降低了复杂度。
</code></pre>
<p>基本构造</p>
<pre><code>AC算法中三个核心函数
* success: 成功转移到另一个状态(goto表和success表)
* failure: 不可顺着字符串跳转的话，则跳转到另一个特定的节点（failure表），从根节点到这个特定的节点的路径恰好是失败前的文本的一部分。
* emits: 命中一个模式串(也称output表)
</code></pre>
<p>举例</p>
<pre><code>以经典的ushers为例，he/she/his/hers 文本为ushers，构造的自动机如图:
</code></pre>
<p><img src="/img/algs_0.png" alt="">
上图省略了到根节点的fail边，完整的自动机如下图:
<img src="/img/algs_1.png" alt=""></p>
<p><strong>匹配过程</strong></p>
<p>自动机从根节点0出发</p>
<p>1 首先按照success表转移(图中实线)。按照文本的指示转移，也就是接受一个u。此时success表中并没有相应路线，转移失败。</p>
<p>2 失败了则按照failure表回去(图中虚线)。按照文本指示，这次接收一个s，转移到状态3</p>
<p>3 成功了继续按success表转移，直到失败跳转步骤2，或者遇到output表中注明的“可输出状态”。此时输出匹配到的模式串，然后将此状态视作普通的状态继续转移。</p>
<p>算法高效之处在于，当自动机接受了“ushe”之后，再接受一个r会导致无法按照success表转移，此时自动机会聪明地按照failure表转移到2号状态，并经过几次转移后输出“hers”。来到2号状态的路不止一条，从根节点一路往下，“h→e”也可以到达。而这个“he”恰好是“ushe”的结尾，状态机就仿佛是压根就没失败过（没有接受r），也没有接受过中间的字符“us”，直接就从初始状态按照“he”的路径走过来一样（到达同一节点，状态完全相同）。</p>
<p>goto表</p>
<p>很简单，trie树知识的话就能了解，goto表就是一棵trie树。把上图的虚线去掉，实线部分就是一棵trie树了。</p>
<p><img src="/img/algs_2.png" alt="">
output表</p>
<p>output表也很简单，与trie树里面代表这个节点是否是单词结尾的构造很像。不过trie树只有叶节点才有output，并且一个叶节点只有一个output。下图违背了这两</p>
<p><img src="/img/algs_3.png" alt=""></p>
<p>以上两个表通过trie构造
<a href="http://www.hankcs.com/nlp/ansj-word-pairs-array-tire-tree-achieved-with-arrays-dic-dictionary-format.html" target="_blank" rel="noopener">Ansj分词双数组Trie树实现与arrays.dic词典格式</a>,
<a href="http://www.hankcs.com/program/java/tire-tree-participle.html" target="_blank" rel="noopener">Trie树分词</a>,<a href="http://www.hankcs.com/program/java/%e5%8f%8c%e6%95%b0%e7%bb%84trie%e6%a0%91doublearraytriejava%e5%ae%9e%e7%8e%b0.html" target="_blank" rel="noopener">双数组Trie树(DoubleArrayTrie)Java实现</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/08/11-Formal-Grammars-of-English/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/08/11-Formal-Grammars-of-English/" itemprop="url">11 Formal Grammars of English</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-08T11:08:44+08:00">
                2018-02-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Formal Grammers of English</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">JackNiu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">46</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">Tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JackNiu</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
