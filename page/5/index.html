<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="SpeechAndLanguageProcessing">
<meta property="og:url" content="http://niuwenchen.github.io/page/5/index.html">
<meta property="og:site_name" content="SpeechAndLanguageProcessing">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SpeechAndLanguageProcessing">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://niuwenchen.github.io/page/5/"/>





  <title>SpeechAndLanguageProcessing</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SpeechAndLanguageProcessing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">translate and learning language model</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/17/3-N-grams/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/17/3-N-grams/" itemprop="url">3_N-grams</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-17T13:14:00+08:00">
                2018-01-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Language Modeling with N-grams</h2>
<p>本章主要介绍一些关于预测单词的基本概念</p>
<pre><code>Please turn your homework ...
后面的词或许是in, over 但不可能是refigerator或the
essential(要素)
概率是我们在噪音，歧义输入如语音识别，手写识别中区分单词的重要因素

模型：定义概率的模型北城与LM
N-gram: 2-gram(bigram) two words &quot;please turn&quot;
3-gram(trigram): &quot;please turn your &quot;
</code></pre>
<h3>N-grams</h3>
<pre><code>计算p(w|h) ,历史h，预测单词w出现的概率

p(the|its water is so transparent that)	
=C(its water is so transparent that the)/C(its water is so transparent that)

一般序列：N， w1...wn,概率P(w1,w2,....wn)
使用链式概率法则计算
</code></pre>
<p><img src="/img/nlp03.png" alt=""></p>
<pre><code>N-gram 并不是计算整个历史，而是计算最近的历史单词
P(the|Walden Pond's water is so transparent that)
~~ P(the|that)

马尔科夫假设：一个单词仅仅依赖前一个单词，进而计算概率
N-gram(只关注前面N-1个单词)

怎么估计N-gram的概率：最大似然估计 MLE
获取次数，归一化次数生成一个概率
p(wn|w(n-1)) = C(w(n-1)wn)/C(w(n-1)w)
</code></pre>
<p><img src="/img/nlp04.png" alt=""></p>
<pre><code>P(i|&lt;s&gt;) = 0:25 P(english|want) = 0:0011
P(food|english) = 0:5 P(&lt;/s&gt;|food) = 0:68

P(&lt;s&gt; i want english food &lt;/s&gt;)
= P(i|&lt;s&gt;)P(want|i)P(english|want)
P(food|english)P(&lt;/s&gt;|food)
= :25× :33× :0011× 0:5× 0:68
= = :000031

计算概率使用对数概率
p1 × p2 × p3 × p4 = exp(log p1+ log p2+ log p3+ log p4)
</code></pre>
<h3>Evaluating LM</h3>
<pre><code>一种评估方法是 把LM嵌入一个应用中计算应该提高多少
这种方式叫做外部验证（extrinsic evaluation）
外部验证方式较为麻烦，内部验证不需要依赖任何应用
</code></pre>
<p>Perplexity(PP)</p>
<p><img src="/img/nlp05.png" alt=""></p>
<pre><code>根据逆频率，条件概率越高，PP越低
内部验证方法并不能确保提高外部效果
</code></pre>
<h3>Generalization and Zeros</h3>
<pre><code>概率代表特定的事实
随着N-gram中N的增大，模型表现越来越好
</code></pre>
<p>下图展示了根据N-gram模型随机生成样本</p>
<p><img src="/img/nlp06.png" alt=""></p>
<pre><code>如果依赖的句子越长，生成的句子越流畅
是不是训练语料有什么潜在的意义？
分析WSJ语料的N-gram和Shakespeare，估计两个N-Gram模型会有相似之处。
</code></pre>
<p><img src="/img/nlp07.png" alt=""></p>
<pre><code>但是实际上没有任何相似之处，哪怕一些小短语。
就是说如果训练集和测试集不同，那么统计模型没有任何用处。

那就确保训练集和测试集有相同的种类，法律模型选择法律训练集就ok了。

概率为0？
训练集中不存在，测试集中存在。
加入训练集是:
	denied the allegations: 5
	denied the speculation: 2
	denied the rumors: 1
	denied the report: 1	
测试集是：
	denied the offer
	denied the loan
p(offer|denied the )=0!!!
如果任何单词的概率为0，那么整个句子的概率就是0，perplexity无法计算。根据公式，不能除以0.
</code></pre>
<p><strong>Unknown Words</strong></p>
<pre><code>closed vocabulary: 所有的测试集中的单词都会在训练集中出现
out of vocabulary: 增加一个&lt;UNK&gt;

1. Choose a vocabulary (word list) that is fixed in advance.
2. Convert in the training set any word that is not in this set (any OOV word) to the unknown word token &lt;UNK&gt; in a text normalization step.
3. Estimate the probabilities for &lt;UNK&gt; from its counts just like any other regular word in the training set.

下面一个方法是经常在训练中使用的
替换训练集中的单词：根据出现次数替换为&lt;UNK&gt;
替换训练集中所有出现次数小于n的单词为&lt;UNK&gt;
或者选择前V个单词，其他的都换成UNK

但是&lt;UNK&gt;模型对度量是有影响的，
</code></pre>
<h3>Smoothing</h3>
<p><a href="http://blog.csdn.net/baimafujinji/article/details/51297802" target="_blank" rel="noopener">平滑方法参考网址</a>
add-1 smoothing,add-k smoothing,Stupid backoff,Kneser-Ney smoothing</p>
<p>Laplace Smoothing</p>
<pre><code>所有的次数加1，
p(wi)=Ci/N;  ----&gt; P(wi)=Ci+1/N+V  #V words
</code></pre>
<p>Add-K</p>
<pre><code>增加一个k，

反正有很多的平滑方法，需要重新写一篇
</code></pre>
<p>###　Summary</p>
<pre><code>１．Language models offer a way to assign a probability to a sentence or other
２．sequence of words, and to predict a word from preceding words.
３．N-grams are Markov models that estimate words from a fixed window of previous words. N-gram probabilities can be estimated by counting in a corpus　and normalizing (the maximum likelihood estimate).
４．N-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.
５．The perplexity of a test set according to a language model is the geometric　mean of the inverse test set probability computed by the model.
６．Smoothing algorithms provide a more sophisticated way to estimat the probability of N-grams. Commonly used smoothing algorithms for N-grams rely　on lower-order N-gram counts through backoff or interpolation.
７．Both backoff and interpolation require discounting to create a probability distribution.
８．Kneser-Ney smoothing makes use of the probability of a word being a novel	continuation. The interpolated Kneser-Ney smoothing algorithm mixes a　discounted probability with a lower-order continuation　probability
</code></pre>
<p>###　Exercise
NGram 实现</p>
<pre><code>#encoding:utf-8
#@Time : 2018/1/17 16:10
#@Author : JackNiu

from collections import defaultdict as ddict
import itertools
import random
class NGram(object):
	def __init__(self,max_n,words=None):
    	self._max_n = max_n
    	self._n_range = range(1,max_n+1)  # range(1,3) --&gt; 1,2
    	self._counts = ddict(lambda :0)   # empty dict

    	if words is not None:
    	    self.update(words)

	def update(self,words):
    	self.splitWords(words)
    	self._counts[()]+=len(self.words)

    	# count ngrams of all  the given lengths
    	for i,word in enumerate(self.words):
        	for n in self._n_range:
            	if i+n &lt;= len(self.words):
                	ngram_range= range(i,i+n)
                	ngram = [self.words[j] for j in ngram_range]
                	self._counts[tuple(ngram)] +=1
    	print(self._counts)

	def splitWords(self,words):
    	# 将一个句子分开 成为单词word
    	words_=[]

    	for word_0 in words.split(&quot; &quot;):
        	for word_1 in word_0.split(','):
            	for word_2 in word_1.split('.'):
                	words_.append(word_2)
    	self.words= words_

	def probability(self):
    	if len(self.words) &lt;= self._max_n:
        	return self._probability()
    	else:
        	prob =1.0
        	for i in range(len(self.words)-self._max_n+1):
            	ngram = self.words[i:i+self._max_n]
            	prob *= self._probability(ngram)
        	return prob

	def _probability(self,ngram):
    	print(&quot;prob&quot;,ngram)
    	ngram = tuple(ngram)
    	ngram_count = self._counts[ngram]
    	print(self._counts[ngram[:-1]],ngram[:-1])
    	prefix_count = self._counts[ngram[:-1]]
    	prob =0.0
    	if ngram_count and prefix_count:
        	prob = ngram_count/prefix_count
    	print(prob)
    	return prob

	def generate(self,n_words):
    	ngrams = iter(self._counts)
    	unigrams = [x for x in ngrams if len(x) ==1]
    	while True:
        	try:
            	return self._generate(n_words,unigrams)
        	except Exception as e:
            pass
	def _generate(self,n_words,unigrams):
    	'''
    	产生句子的方法：
    	:param n_words:
    	:param unigrams:
    	:return:
    	'''
    	words=[]
    	for i in itertools.repeat(self._max_n):
        	print(i)
        	if i==1:
            	prefix=()
        	else:
            	prefix = tuple(words[-i+1:])

        	threshold = random.random()
        	total =0.0
        	print(threshold)
        	for unigram in unigrams:
            	total += self._probability(prefix+unigram)
            	if total &gt;= threshold:
                	words.extend(unigram)
                	break
        	print(words)
        	if len(words) == n_words:
            	return words
        	if total == 0.0:
            	raise RuntimeError('impossible sequence')






str=&quot;Hello Jack Are you ok?&quot;
ngram =NGram(3,str)
print(&quot;结果：&quot;,ngram.generate(2))

3
0.08865073743798602
prob ('Jack',)
5 ()
0.2
['Jack']
3
0.9364222834379184
prob ('Jack', 'Jack')
1 ('Jack',)
0.0
prob ('Jack', 'you')
1 ('Jack',)
0.0
prob ('Jack', 'ok?')
1 ('Jack',)
0.0
prob ('Jack', 'Are')
1 ('Jack',)
1.0
['Jack', 'Are']
结果： ['Jack', 'Are']
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/15/undirect-graph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/15/undirect-graph/" itemprop="url">Undirect graph</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-15T20:07:25+08:00">
                2018-01-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3>Graphs</h3>
<p>A graph is a set of <em>vertices</em> and a collection of <em>edges</em> that each connect a pair of vertices. We use the names 0 through V-1 for the vertices in a V-vertex graph</p>
<p><img src="/pict/graph.png" alt=""></p>
<h3>Glossary:Here are some definitions that we use</h3>
<ul>
<li>A self-loop is an edge that connects a vertex to itself</li>
<li>Two edges are parallel if they connect the same pair of vertices</li>
<li>When an edge connects two vertices, we say that the vertices are adjacent to one another and that the edge is incident on both vertices.</li>
<li>The degree of a vertex is the number of edges incident on it.</li>
<li>A subgraph is a subset of graph's( and associated vertices) that constitutes a graph.</li>
<li>A cycle is a path (with at least one edge) whose first and last vertices are the same. A simple cycle is a cycle with no repeated edges or vertices (expect thr requisite repetition of the first and last vertices)</li>
<li>The length of a path or a cycle is its number of edges.</li>
<li>We say that one vertex is connected to another if there exists a path that contains both of them.</li>
<li>A graph is connected if there is a path from every vertex to every other vertex.</li>
<li>A Graph that is bot connected consists of sa set of connected components,which are maximal connected subgraphs.</li>
<li>An acyclic(无环) graph is a graph with no cycles.</li>
<li>A tree is an acyclic connected graph.</li>
<li>A forest is a disjoint set of trees.</li>
<li>A spanning tree of a connected graph is a subgraph that contains all of that graph's vertices and is a single tree. A spanning forest of a graph is the union of the spanning trees of its connected components.</li>
<li>A bipartite graph is a graph whose vertices we can divide into two sets such that all edges connect a vertex in one set with a vertex in the other set.</li>
</ul>
<p><img src="/pict/graph-anatomy.png" alt="">
<img src="/pict/tree.png" alt="">
<img src="/pict/forest.png" alt=""></p>
<h3>Undirected graph data type.</h3>
<p>We implement the following undirected graph API.</p>
<p><img src="/pict/graph-api.png" alt=""></p>
<p>The key method adj() allows client code to iteratr through the vertices adjacent to a given vertex. Remarkably, we can build all of the algorithms that we consider in this section on the basic abtraction embodied in adj()</p>
<p>we prepare the test data <a href="https://algs4.cs.princeton.edu/41graph/tinyG.txt" target="_blank" rel="noopener">tinyG.txt</a>,<a href="https://algs4.cs.princeton.edu/41graph/mediumG.txt" target="_blank" rel="noopener">mediumG.txt</a>,and <a href="https://algs4.cs.princeton.edu/41graph/largeG.txt" target="_blank" rel="noopener">largeG.txt</a>,using the following input file format</p>
<p><img src="/pict/graph-input.png" alt=""></p>
<p><a href="https://algs4.cs.princeton.edu/41graph/GraphClient.java.html" target="_blank" rel="noopener">GraphClient.java</a> contains typical graph-processing code.</p>
<h3>Graph representation.</h3>
<p>We use the adjacency-lists representation, where we maintain a vertex-indexed array of lists of the vertices connected by an edge to each vertex.</p>
<p><img src="/pict/adjacency-lists.png" alt=""></p>
<p><a href="https://algs4.cs.princeton.edu/41graph/Graph.java.html" target="_blank" rel="noopener">Graph.java</a> implements the graph API using the adjacency-lists representation. <a href="">AdjMatrixGraph.java</a> implements the same API using the adjacency-matrix representation.</p>
<h3>Depth-first search.</h3>
<p>Depth-first search is a classic recursive method for systematically examining each of  the vertices and edges in a graoh. To visit a vertex</p>
<ul>
<li>Matk it as having been visited</li>
<li>Visit(recursively) all the vertices that are adjacent to it and that have not yet been marked.</li>
</ul>
<p><a href="">DepthFirstSearch.java</a> implements this approach and the following API:</p>
<p><img src="/pict/search-api.png" alt="">
<img src="/pict/02.jpg" alt=""></p>
<h3>Finding Paths</h3>
<p>It is easy to modify depth-first search to not only determine whether there exists a path between two given vertices but to find such a path
(if one exists). we seek to implements the following API:</p>
<p><img src="/pict/paths-api.png" alt=""></p>
<p>To accomplish this,we remember the edge v-w that takes us to each vertex w for the first time by setting edgeTo[w] to v.In other words,
v-w is the last edge on the known path from s to w. The result of search is a tree rooted at the source; edgeTo[] is a parent-link representation of that tree.
<a href="">DepthFirstPaths.java</a> implement this approach.</p>
<h3>Breadth-first search</h3>
<p>Depth-first search finds some pat from a source verrex s to a target vertex v. We are often interested in finding the shortest such
path(one with a minimal number of edges). Breadth-first search is a classic method based on this goal. To find a shortest path from
s to v, we start at s and check for v among all the vertices that we can reach by following one edge, then we check for v among all the vertices
that we can reach from s by following two edges,and so forth.</p>
<p>To implement this strategy, we maintain a queue of all vertices that have been marked but whose adjacency lists have not been checked. we
put the source vertex on the queue,then perform the following steps until the queue is empty:</p>
<ul>
<li>remove the next vertex v from the queue</li>
<li>put onto the queue all unmarked vertices that are adjacent to v and mark them.
<a href="">BreadthFirstPaths.java</a> is an implememntation of the Paths API that finds a shotest paths. It relies on <a href="">Queue.java</a> for the FIFO queue.</li>
</ul>
<h3>联通图</h3>
<p><a href="">Connected components</a>. Our next direct application of depth-first search is to find the connected components of a graph. Recall from Section 1.5 that &quot;is connected to&quot; is an equivalence relation that divides the vertices into equivalence classes (the connected components).
For this task, we define the following API:</p>
<p><img src="/pict/cc-api.png" alt=""></p>
<h3>Symbol graphs.</h3>
<p>Typical applications involve processing graphs using strings, not integer indices, to define and refer to vertices.
To accommodate such applications, we define an input format with the following properties:</p>
<ul>
<li>Vertex names are strings.</li>
<li>A specified delimiter separates vertex names (to allow for the possibility of spaces in names).</li>
<li>Each line represents a set of edges, connecting the first vertex name on the line to each of the other vertices named on the line.</li>
</ul>
<p>The input file <a href="https://algs4.cs.princeton.edu/41graph/routes.txt" target="_blank" rel="noopener">routes.txt</a> is a small example.</p>
<p><img src="/pict/routes.png" alt="">
The input file <a href="https://algs4.cs.princeton.edu/41graph/movies.txt" target="_blank" rel="noopener">movies.txt</a> is a larger example from the Internet Movie Database.
This file consists of lines listing a movie name followed by a list of the performers in the movie.</p>
<p><img src="/pict/movies.png" alt=""></p>
<p>API. The following API allows us to use our graph-processing routines for such input files.</p>
<p><img src="/pict/symbol-graph-api.png" alt=""></p>
<p>Implementation. <a href="https://algs4.cs.princeton.edu/41graph/SymbolGraph.java.html" target="_blank" rel="noopener">SymbolGraph.java</a> implements the API. It builds three data structures:</p>
<pre><code>A symbol table st with String keys (vertex names) and int values (indices)

An array keys[] that serves as an inverted index, giving the vertex 
name associated with each integer index
A Graph G built using the indices to refer to vertices
</code></pre>
<p><img src="/pict/symbol-graph.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/12/2-Regular-expressions-Text-normalization-Edit-Distance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/12/2-Regular-expressions-Text-normalization-Edit-Distance/" itemprop="url">2 Regular expressions,Text normalization,Edit Distance</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-12T16:05:19+08:00">
                2018-01-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3>2.1 Regular Expressions</h3>
<p>描述一些正则表达式的写法以及demo，因为学不会，所以不写了。</p>
<pre><code>如下是一些使用RE进行机器翻译的过程

s/.* I'M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE\1/
s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \1/
s/.* all .*/IN WHAY WAY/
s/.* always .*/ CAN YOU THINK OF A SPECIFIC EXAMPLE/

因为多种替换方式的存在，这些候选集翻译需要被评分并且排序
</code></pre>
<h3>Words and Corpora(资料)</h3>
<pre><code>lemmatization(词形还原)
tokens,即文本的词总数 
types, 即文本的不同词形总数
</code></pre>
<h3>Text Normalization</h3>
<pre><code>自然语言处理过程中，文本需要被正规化
1. 分割/标记单词
2. 规范单词格式
3. 分隔句子
</code></pre>
<p>Unix 工具</p>
<pre><code>Shapespare  sh.txt
tr -sc 'A-Za-z' '\n' &lt; sh.txt
分隔句子，按照单词进行分隔
tr -sc 'A-Za-z' '\n' &lt;sh.txt |sort | uniq -c
</code></pre>
<p>单词标记规范</p>
<pre><code>tokenization: 分隔标记
normalization: 将单词转换成标准格式
clitic: 附着词素we're --&gt; we are

Penn Treebank tokenization: 标准分隔

Input: &quot;The San Francisco-based restaurant,&quot;they said,&quot;doesn't charge $10&quot;.
Output: &quot;/The/San/Fransisco-based/restaruant/,/&quot;/they/said/,/&quot;/does/n't/charge/$/10/&quot;/.

Case folding 

在实际中，tokenization/normalization 使用基于RE的确定算法。
</code></pre>
<p>word Segmentation in chinses: MaxMatch 算法</p>
<pre><code>greedy search: maximum matching, MaxMatch


算法描述
function MaxMatch(sentence,dictionary D) returns word sequence W

if sentence if empty
	return empty list
for i &lt;- length(sentence) downto l
	firstword = first i chars of sentence
	remainder = rest of sentence
	if InDictionary(firstword,D)
		return list(firstword,MaxMatch(remainder,dictionary))

# no word was found , so make a one-character word
firstword = first char of sentence
remainder = rest of sentence
return list(firstword,MaxMatch(remainder,dictionary D))

Input: 他特别喜欢北京烤鸭
Output: 他   特别   喜欢    北京烤鸭

可以通过word error rate 度量准确度
MaxMatch在汉语中表现良好，但是对于未登录词的效果就比较一般了。
</code></pre>
<p>Lemmatization and Stemming (词干)</p>
<p>The Porter Stemmer（波特词干分析器）</p>
<pre><code>ATIONAL--&gt;ATE(relational --&gt; relate)
ING--&gt; e  motoring --&gt;moitor
SSES----&gt;SS (grasses --&gt; grass)
</code></pre>
<p>sentence segmentation</p>
<h3>Minimum Edit Distance</h3>
<p>Edit distance 衡量字符串的相似性。</p>
<pre><code>最小编辑距离定义为最小编辑操作(插入，删除，替换)
</code></pre>
<p><img src="/img/nlp01.png" alt=""></p>
<pre><code>Levenshtein距离是最简单的权重因子，可以衡量每个操作的操作。
</code></pre>
<p>Minimum Edit Distance Algorithm</p>
<pre><code>bynamic programming: Viterbi and fordwar algorithms
</code></pre>
<p><img src="/img/nlp02.png" alt=""></p>
<pre><code>source String: X  n
target String: Y  m
D(i,j): X[1..i] and Y[1...j]
edit distance: X,Y  --&gt; D(n,m)

D[i,j]=min{D[i-1,j]+del-cost(source[i]),D[i,j-1]+ins-cost(target[j]),D[i-1,j-1]+sub-cost(source[i]mtarget[j])}
ins-cost(*)=del-cost(*)=1 

D[i,j]=min{D[i-1,j]+1,D[i,j-1]+1,D[i-1,j-1]+{2: if source[i]!=target[j]; 0: if source[i]= tagrte[j]}}
</code></pre>
<p><img src="/img/nlp.png" alt=""></p>
<p><a href="http://www.dreamxu.com/books/dsa/dp/edit-distance.html" target="_blank" rel="noopener">http://www.dreamxu.com/books/dsa/dp/edit-distance.html</a></p>
<p><a href="https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm" target="_blank" rel="noopener">https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm</a></p>
<h3>Summary</h3>
<p>主要介绍基本工具：</p>
<pre><code>RE
text normalization: word segmentation/normalization/sentence segmentation/stem
minimum edit distance:
</code></pre>
<ul>
<li>The regular expression language is a powerful tool for pattern-matching</li>
<li>Basic oprtations in regular expressions include concatenation of symbols, disjunction of symbols,counters ,anchors</li>
<li>Word tokenization and normalization are generally done by cascades of simple regular expressions or finite automata</li>
<li>The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes.It does not have high acuracy but may be useful for some tasks</li>
<li>The minimum edit distance between two strings is the minimum number of operations is takes to edit one into other. Minimum edit distance can be computed by dynamic programming,which also results in an alignment of the two strings.</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/12/Introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/12/Introduction/" itemprop="url">Introduction</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-12T11:32:58+08:00">
                2018-01-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Introudction</h2>
<p>computer speech and language processing or human language technology or natural language processing or computational linguistics.</p>
<h3>1.1  Knowledge In Speech And language Processing</h3>
<p>HAL must use structural knowledge to properly string together the words that consitute its response. For example,HAL must know that the following sequence of words will not make sense to Dave, despite the fact that it contains precisely the same set of words as the original.</p>
<pre><code>I'am I do, sorry that afraid Dave I can't
</code></pre>
<p>The knowledge needed to order and group words together comes under the hedaing of syntax(句法)</p>
<pre><code>Now consider a question awsering dealing with the following question:
How much Chinese silk was exported to Western Europe by the end of the 18th century?
</code></pre>
<p>we need lexical semantics(词汇语义学)</p>
<p>To summarize, engaging in complex language behavior various kinds of knowledge of language:</p>
<h2></h2>
<ul>
<li>Phonetics and Phonology- knowledge about linguistic sounds(语音学和音韵学-关于语言声音的知识)</li>
<li>Morphology - knowledge of the meaningful components of words(形态学-词汇意义成分的知识 )</li>
<li>Syntax- knowledge of the structural relationships between words(句法-词汇结构关系的知识)</li>
<li>Semantics - knowledge of meaning(语义学——意义的知识)</li>
<li>Pragmatics - knowledge of the relationship of meaning to the goals and intentions of the speaker(语用学——理解意义与说话者意图和意图的关系)</li>
<li>Discource - knowledge about linguistic units larger than a single utterance（话语</li>
</ul>
<h2></h2>
<p>###1.2 Ambiguity （歧义）
we say some input is <strong><em>ambiguous</em></strong> if there are multiple alternative linguistic structures that can be built for it.</p>
<h3>1.3 Models and algorithms</h3>
<p>Among the most important models are <strong>State Machines,rule systems,logic,probabilistic models, vector-space models</strong>
.....</p>
<h3>1.4 Language,Thought,and Understanding</h3>
<h3>1.5 The State of the Art</h3>
<h3>1.6  Some Brief History</h3>
<p>.....</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/11/README-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/11/README-md/" itemprop="url">说明</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-11T18:46:40+08:00">
                2018-01-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>根据《speech and language processing 2》 和第三版样本进行学习。</p>
<p><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">第三版链接</a></p>
<p><a href="http://www.cs.cmu.edu/afs/cs/user/tbergkir/www/11711fa17/" target="_blank" rel="noopener">NLP算法</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">JackNiu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">Tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JackNiu</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
