<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="SpeechAndLanguageProcessing">
<meta property="og:url" content="http://niuwenchen.github.io/page/3/index.html">
<meta property="og:site_name" content="SpeechAndLanguageProcessing">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SpeechAndLanguageProcessing">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://niuwenchen.github.io/page/3/"/>





  <title>SpeechAndLanguageProcessing</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SpeechAndLanguageProcessing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">translate and learning language model</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/06/9-Hidden-Markov-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/06/9-Hidden-Markov-Models/" itemprop="url">9 Hidden Markov Models</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-06T10:59:53+08:00">
                2018-02-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>##Hidden Markov Models
HMM是一个序列模型。对一句话定义一个标签或者类，因此将一个可观测的序列映射为一个label序列。 HMM是一个概率序列模型: 给定一个序列，计算可能的label序列的概率分布，选择最好的一个label 序列。</p>
<p>本章介绍三个算法Viterbi 算法，Forward 算法， Baum-Welch 或者Em算法。</p>
<h2>Markov Chains</h2>
<p><img src="/img/nlp9_0.png" alt=""></p>
<pre><code>Q=q1q2....qN    a set of N states
A=a01a02...an1...ann  a trainsation probability matrix A, each aij representing the probability of moving from state i to state j . 
q0,qF 		a special start state and end state that are not associaed with observations
</code></pre>
<p><strong>Markov Assumption:</strong></p>
<p>$$ P(q_{i}|q_1..q_{i-1})=P(q_{i}|q_{i-1})$$</p>
<p>$$ \pi =\pi_{1},\pi_{2},..\pi_{N}$$ , an initial probability distribution over states.也就是说马尔科夫链从状态i开始，其余的状态将会是0.</p>
<p>因此，state 1 是第一个状态，可以被表示为a01 或者$\pi_{1}$。每一个$\pi_{i}$代表概率 P(qi|START)。</p>
<p><img src="/img/nlp9_1.png" alt=""></p>
<h2>The Hidden Markoc Model</h2>
<p><img src="/img/nlp9_2.png" alt=""></p>
<p>HMM 有两个特殊的假设</p>
<pre><code>马尔科夫假设，状态转移概率只依赖于前一个状态
输出假设: 观察状态的概率只依赖与当前的隐含状态
</code></pre>
<p><img src="/img/nlp9_3.png" alt=""></p>
<p>HMM三个问题, 隐马隐含状态序列，概率矩阵参数，输出序列</p>
<ul>
<li>Likelihood</li>
<li>Decoding</li>
<li>Learning</li>
</ul>
<h2>Likelihood Computation: The Forward Algorithm</h2>
<p>第一个问题计算一个特定的观测序列的概率。比如，给出ice-cream eating HMM，那么序列 3 1 3 出现的概率是多少</p>
<p>**Computing Likelihood **</p>
<p>Given an HMM $\lambda =(A,B)$,and an observation sequence O,determine the likelihood $P(O|\lambda) $</p>
<p>首先，对于HMM来说，每一个隐藏状态仅仅产生一个观测，因此，隐含状态和观测状态有相同的长度。</p>
<p><img src="/img/nlp9_4.png" alt=""></p>
<p>但是实际上，上面的计算过程都是在我们基于已经知道隐含状态序列的情况下进行的，实际上是不知道的。 我们需要计算的是ice-cream 出现的概率。</p>
<p><img src="/img/01.png" alt=""></p>
<p>按照上式的理解，如果需要一个结果，需要将整个遍历以便，一种可能的状态如下</p>
<p>$$P(3 1 3,hot hot cold)=P(hot|start)\times P(hot|hot) \times P(cold|hot) \times P(3|hot) \times P(1|hot)\times P(3|cold)$$</p>
<p>计算了一种假设的，那么对于一个序列可能出现的所有情况做一个总和</p>
<p>$$P(3,1,3)=P(3,1,3,cold cold cold)+P(3 1 3,cold cold hot)+P(3 1 3,hot hot cold)+...$$</p>
<p>对于一个HMM 有N个hidden 状态和T个观测状态，有$N^{T}$ 个可能的隐含序列。</p>
<p>使用一种高效的算法 <strong>forward algorithm.</strong>,是<strong>dynamic programing</strong>算法。</p>
<p>每一个前向算法的单元格$\alpha _{t}(j)$代表看到t时刻观察值时处于状态j的概率，</p>
<p>$$\alpha_{t}(j)= P(o_{1},...o_{t},q_{t}=j|\lambda )$$</p>
<p>qt=j 代表&quot;第t时刻的状态是state j&quot;。</p>
<pre><code>we compute this probability at(j) 是对所有路径上可以到达这个网格的路径进行求和。
</code></pre>
<p><img src="/img/nlp9_5.png" alt=""></p>
<p>根据图片的描述，整个公式可以重写为</p>
<p><img src="/img/02.png" alt=""></p>
<p>$\alpha_{t-1}(i)$ previous forward path probability</p>
<p>$a_{ij}$ transition probability</p>
<p>$b_{j}(o_{t})$ state observation likelihood</p>
<p><img src="/img/nlp9_6.png" alt=""></p>
<p>计算过程</p>
<p>1 Initialization:</p>
<p>$\alpha_{1}(j)=a_{oj}b_{j}(o_{1})$，例如上式中的$\alpha_1(2)=P(3|H)*P(H|START)$,这里的a01 就是start--&gt;1的转变</p>
<p>2 递归计算</p>
<p><img src="/img/03.png" alt=""></p>
<p>3 决策</p>
<p><img src="/img/04.png" alt=""></p>
<p>前向算法实现
<img src="/img/nlp9_7.png" alt=""></p>
<h2>Decoding: The Viterbi Algorithm</h2>
<pre><code>Decoding: Given as input an HMM ,and a sequence of observation O,find
the most probable sequence of states Q=q1q2...qT
</code></pre>
<p>需要发现最好的隐含序列，对于可能的隐藏状态序列(HHH,HHC,HCH,etc.)，可以运行前向算法，计算最后的概率，概率最大的就是最好的序列。</p>
<p>但是，最好的decoding方法是Viterbi算法。Viterbi也是一个动态编程过程。
<img src="/img/nlp9_8.png" alt=""></p>
<pre><code>t=1 时刻就可以用max，从t=2时刻开始进行max选择。
</code></pre>
<p><img src="/img/nlp9_9.png" alt=""></p>
<p><img src="/img/nlp9_10.png" alt=""></p>
<h2>HMM Training:The Forward-Backward Algorithm</h2>
<p>HMM 的参数计算。</p>
<pre><code>Learning: Given an observation sequence O and the set of possible 
states in the HMM ,learn the HMM parameters A and B
The algorithm will let us train both the transition probabilities A 
and the emission probabilities B of the HMM
</code></pre>
<p>EM算法就暂时先不看了。</p>
<h2>Summary</h2>
<pre><code>This chapter introducted the hidden Markov model for probabilistic 
sequence classification.

hidden Markov models are a way of relating a sequence of 
observations to a sequence of hidden classes or hidden states that 
explain the observations

The process of discovering the sequence of hidden states,given the 
sequence of observations, is known as decoding or inference. The 
Viterbi algorithm is commonly used for decoding 

The parameters of an HMM are the A transition probability matrix and 
the B  observation likelihood matrix. Both can be trained with the
Baum-Welch or forward-backward algorithm.
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/05/Hanlp分词技术/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/05/Hanlp分词技术/" itemprop="url">Hanlp分词技术</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-05T23:14:50+08:00">
                2018-02-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>分词技术</h2>
<h3>HanLP 中的WordNet</h3>
<pre><code>public WordNet(char[] charArray)
按照句子的分析过程，每一个词都可能有引申。
每一个词作为一个节点，其都是一个链表形式。
头尾为B ,E

刚开始所有的顶点都是空的，
0:[WordNode{word='始##始', realWord=' ', attribute=begin 2514605 }]
1:[]
2:[]
3:[]
4:[]
5:[]
6:[WordNode{word='末##末', realWord=' ', attribute=end 2514605 }]

public WordNet(char[] charArray, List&lt;Vertex&gt; vertexList)
初始构造，只是往里面添加了顶点，也就是为顶点链表添加节点的过程

添加的过程是按照 初始句子中给出的所有顶点进行添加。

商品和服务: 
vertext[0].add(商)
</code></pre>
<p>GenerateWordNet 生成一元词网</p>
<pre><code>按照上面的描述将所有的可以成词的词组成词网，具体过程就是查询词典
0:[WordNode{word='始##始', realWord=' ', attribute=begin 2514605 }]
1:[WordNode{word='商', realWord='商', attribute=vg 607 v 198 }, WordNode{word='商品', realWord='商品', attribute=n 2209 }]
2:[WordNode{word='品', realWord='品', attribute=ng 563 }]
3:[WordNode{word='和', realWord='和', attribute=cc 141341 }, WordNode{word='和服', realWord='和服', attribute=n 34 }]
4:[WordNode{word='服', realWord='服', attribute=v 564 }, WordNode{word='服务', realWord='服务', attribute=vn 11789 v 2898 }]
5:[WordNode{word='务', realWord='务', attribute=vg 209 }]
6:[WordNode{word='末##末', realWord=' ', attribute=end 2514605 }]

里面的顶点是:
按照句子的长度，分成length个顶点，每个顶点都是一个链表
[WordNode{word='始##始', realWord=' ', attribute=begin 2514605 }]
[WordNode{word='商', realWord='商', attribute=vg 607 v 198 }, WordNode{word='商品', realWord='商品', attribute=n 2209 }]
[WordNode{word='品', realWord='品', attribute=ng 563 }]
[WordNode{word='和', realWord='和', attribute=cc 141341 }, WordNode{word='和服', realWord='和服', attribute=n 34 }]
[WordNode{word='服', realWord='服', attribute=v 564 }, WordNode{word='服务', realWord='服务', attribute=vn 11789 v 2898 }]
[WordNode{word='务', realWord='务', attribute=vg 209 }]
[WordNode{word='末##末', realWord=' ', attribute=end 2514605 }]

词网生成。
</code></pre>
<p>开始分词</p>
<pre><code>首先建立vertex之间的联系，就是首尾相连，之前是所有的顶点各自为一个list，现在是一个总的list。每个vertex之间多了约束

[WordNode{word='始##始', realWord=' ', attribute=begin 2514605 }, WordNode{word='商品', realWord='商品', attribute=n 2209 }, WordNode{word='和', realWord='和', attribute=cc 141341 }, WordNode{word='服务', realWord='服务', attribute=vn 11789 v 2898 }, WordNode{word='末##末', realWord=' ', attribute=end 2514605 }]
</code></pre>
<p>上面是一些基本的介绍过程，下面结合具体的分词算法进行分析</p>
<h2>分词算法</h2>
<pre><code> List&lt;Term&gt; termList = HanLP.segment(&quot;南京市长江大桥&quot;);
 System.out.println(termList);

return new ViterbiSegment();   // Viterbi分词器是目前效率和效果的最佳平衡
</code></pre>
<p>Viterbi</p>
<p><img src="/img/hanlp_0.png" alt=""></p>
<pre><code>best_score[0]=0
for each node in the graph(ascending order)
	best_score[node] = inf
	for each incoming edge of node:
		score = best_score[edge.prev_node]+ edge.score
		if score &lt; best_score[node]
			best_score[node] = score
			best_edge[node]=edge

最后得出的
best_score =(0.0,2.5,1.4,3.7) 是每一个节点的分数
best_edge = (null, e1,e2,e5),以node为准，进入这个节点的边

Backward Step

best_path=[]
next_edge = best_edge[best_edge.length -1]
while next_edge !=NULL
	add next_edge to best_path
	next_edge = best_edge[next_edgee.prev_node]
reverse best_path

best_path=[e2,e5]
</code></pre>
<p><a href="http://www.phontron.com/slides/nlp-programming-en-03-ws.pdf" target="_blank" rel="noopener">http://www.phontron.com/slides/nlp-programming-en-03-ws.pdf</a></p>
<pre><code>private static  List&lt;Vertex&gt; viterbi(WordNet wordNet)
{
    LinkedList&lt;Vertex&gt; nodes[] = wordNet.getVertexes();
    LinkedList&lt;Vertex&gt; vertexList = new LinkedList&lt;&gt;();
    for(Vertex node : nodes[1])
    {
        // 更新初始节点到首节点的联系。其他的不用更新吗
        node.updateFrom(nodes[0].getFirst());
    }
    for (int i = 1; i &lt; nodes.length - 1; ++i)
    {
        LinkedList&lt;Vertex&gt; nodeArray = nodes[i];
        if (nodeArray == null) continue;
        for (Vertex node : nodeArray)
        {
            if (node.from == null) continue;
            for (Vertex to : nodes[i + node.realWord.length()])
            {
//                    double weight = from.weight + MathTools.calculateWeight(from, this);
//                    if (this.from == null || this.weight &gt; weight)
//                    {
//                        this.from = from;
//                        this.weight = weight;
//                    }
                to.updateFrom(node);
            }
        }
    }
    System.out.println(&quot; 这里应该有一个best_score, 和best_path, 然后从backward上寻找最佳路径&quot; +
            &quot;forward 方式已经更新了前后连接方式&quot;);
    Vertex from = nodes[nodes.length - 1].getFirst();
    System.out.println(&quot;from:0 &quot;+from);
    while (from != null)
    {
        vertexList.addFirst(from);
        from = from.from;
        System.out.println(&quot;from:1 &quot;+from);
    }
    return vertexList;
}
</code></pre>
<p>根据前向算法计算best_score和best_path,然后根据后向算法计算best_path,注意 Viterbi算法一定是按照node来进行算法的持续，不是边</p>
<p>NLP分词</p>
<pre><code>List&lt;Term&gt; termList = NLPTokenizer.segment(&quot;中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程&quot;);
System.out.println(termList);


public static final Segment SEGMENT = HanLP.newSegment().enableNameRecognize(true).enableTranslatedNameRecognize(true)
        .enableJapaneseNameRecognize(true).enablePlaceRecognize(true).enableOrganizationRecognize(true)
        .enablePartOfSpeechTagging(true);



NLP分词实际上也是HMM分词，只是增加了全部命名实体识别和词性标注过程，速度比较慢，并且有误识别的情况。
</code></pre>
<p>索引分词</p>
<pre><code>索引分词 IndexTokenizer 是面向搜索引擎的分词器，能够对长词全切分，另外通过 term.offset 可以获取单词在文本中的偏移量。调用方法如下:

	List&lt;Term&gt; termList = IndexTokenizer.segment(&quot;南京市长江大桥&quot;);
    for (Term term : termList)
    {
        System.out.println(term + &quot; [&quot; + term.offset + &quot;:&quot; + (term.offset + term.word.length()) + &quot;]&quot;);
    }
南京市/ns [0:3]
南京/ns [0:2]
长江/ns [3:5]
大桥/n [5:7]


也是在HMM-Viterbi的基础上进行了其他扩展

public static final Segment SEGMENT = HanLP.newSegment().enableIndexMode(true);
</code></pre>
<p>繁体分词</p>
<pre><code>繁体分词 TraditionalChineseTokenizer 可以直接对繁体进行分词，输出切分后的繁体词语。调用方法如下:

List&lt;Term&gt; termList = TraditionalChineseTokenizer.segment(&quot;大衛貝克漢不僅僅是名著名球員，球場以外，其妻為前辣妹合唱團成員維多利亞·碧咸，亦由於他擁有突出外表、百變髮型及正面的形象，以至自己品牌的男士香水等商品，及長期擔任運動品牌Adidas的代言人，因此對大眾傳播媒介和時尚界等方面都具很大的影響力，在足球圈外所獲得的認受程度可謂前所未見。&quot;);
System.out.println(termList);
</code></pre>
<p>极速词典分词</p>
<pre><code>极速分词是词典最长分词，速度极其快，精度一般。调用方法如下:

使用的算法是 《Aho Corasick自动机结合DoubleArrayTrie极速多模式匹配》

String text = &quot;江西鄱阳湖干枯，中国最大淡水湖变成大草原&quot;;
System.out.println(SpeedTokenizer.segment(text));
</code></pre>
<p>N-最短路径分词</p>
<pre><code>最短路分词器 NShortSegment 比最短路分词器( DijkstraSegment )慢，但是效果稍微好一些，对命名实体识别能力更强。调用方法如下:
</code></pre>
<p>Dijkstra分词</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/05/专题推荐0205/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/05/专题推荐0205/" itemprop="url">专题推荐0205</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-05T13:39:13+08:00">
                2018-02-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>专题推荐</h2>
<p>专题推荐 ------&gt; 衣食住行</p>
<p>家庭人口判断？</p>
<p>算法本身根据不一样的业务属性，</p>
<p>基于现有的数据进行优化</p>
<p>刘珊珊</p>
<p>王迪</p>
<p>吴尚波 / 高响 /李竹红</p>
<p>吴久清</p>
<p>直播导流</p>
<pre><code>在某个专题里面，专题的其他节目全部推荐； 如果没有推荐，就推荐相关
</code></pre>
<p>我的直播推荐</p>
<pre><code>几个频道的节目，5； 
</code></pre>
<p>我的轮播</p>
<pre><code>个性化推荐，轮播方式。
</code></pre>
<p>专题内容</p>
<pre><code>专题</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/05/数学之美-自然语言处理-从规则到统计/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/05/数学之美-自然语言处理-从规则到统计/" itemprop="url">数学之美 自然语言处理 从规则到统计</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-05T09:56:37+08:00">
                2018-02-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>自然语言处理 从规则到统计</h2>
<p><img src="/img/math2_0.png" alt=""></p>
<p>句法分析，先看下面一个简单的句子</p>
<pre><code>徐志摩喜欢林徽因
</code></pre>
<p>这个句子分为主语、谓语和句号三部分，对每个部分进行分析，得到下面的句法分析树(Parse Tree)。</p>
<p><img src="/img/math2_1.png" alt=""></p>
<pre><code>句子----&gt; 主语谓语句号
主语----&gt; 名词
谓语----&gt; 动词  名词短语
名词短语----&gt; 名词
名词----&gt; 徐志摩
动词----&gt; 喜欢
名词----&gt; 林徽因
句号----&gt; 。
</code></pre>
<p>20世纪80年代以前，自然语言处理工作中的文法规则都是人写的，这和后来采用机器总结的做法大不相同。直到2000年后，很多公司，比如机器翻译公司SysTran,还靠人来总结文法规则。</p>
<h2>从规则到统计</h2>
<p>上世纪70年代，基于统计的方法的核心模型是通信系统加隐含马尔科夫模型。</p>
<p>今天，几乎不再有科学家宣称自己是传统的基于规则方法的捍卫者。自然语言处理的规则也从单纯的句法分析和语义理解，变成了非常贴近应用的机器翻译、语音识别、文本到数据库自动生成、数据挖掘和知识的获取等等。</p>
<h2>小结</h2>
<p>基于统计的自然语言处理方法，在数学模型上和通信是相通的，甚至就是相通的。因此，在数学意义上自然语言处理又和语言的初衷---通信联系在一起了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/05/数学之美-文本和语言-数字和信息/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/05/数学之美-文本和语言-数字和信息/" itemprop="url">数学之美 文本和语言 数字和信息</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-05T09:52:31+08:00">
                2018-02-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>文字和语言Vs 数字和信息</h2>
<p>通信的原理和信息传播的模型</p>
<p>信源编码和最短编码</p>
<p>编码的规则、语法</p>
<p>聚类</p>
<p>校验位</p>
<p>双语对照文本、语料库和机器翻译</p>
<p>多义性和利用上下文消除歧义性。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Seq2Seq中的函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Seq2Seq中的函数/" itemprop="url">Seq2Seq中的函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T14:28:46+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Seq2Seq 中的函数</h2>
<pre><code>tf.constant(value): 将一个常数转化为tensor
tf.cast(x,dtype): 将x的数据格式转化成dtype

a = tf.Variable([1,0,0,1,1])
b = tf.cast(a,dtype=tf.bool)
sess = tf.InteractiveSession()
sess.run(tf.initialize_all_variables())
print(sess.run(b))
[ True False False  True  True]



1. src_vocab_table.lookup(tf.constant(eos))


2. tf.contrib.lookup.index_table_from_tensor(),构建单词索引对
	
	mapping_strings = tf.constant([&quot;emerson&quot;, &quot;lake&quot;, &quot;palmer&quot;])
	table = tf.contrib.lookup.index_table_from_tensor(
  	mapping=mapping_strings, num_oov_buckets=1, default_value=-1)
	features = tf.constant([&quot;emerson&quot;, &quot;lake&quot;, &quot;and&quot;, &quot;palmer&quot;])
	ids = table.lookup(features)

	with tf.Session() as sess:
		tf.tables_initializer().run()
		print(ids.eval() )
		[0 1 3 2]
</code></pre>
<p>Dataset 与Iterator</p>
<pre><code>Dataset可以看作是相同类型元素的有序列表，在实际使用时，单个元素可以是向量，也可以是字符串，图片，tuple或者dict

是TextLineDataset TFRecordDataset FixedLengthRecorddataset 的父类

dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))
这个dataset中含有5个元素，分别是1.0, 2.0, 3.0, 4.0, 5.0。

如何取出这个dataset中的元素？方法是从dataset中实例化一个Iterator，然后对Iterator进行迭代

iterator = dataset.make_one_shot_iterator()
one_element = iterator.get_next()
with tf.Session() as sess:
	for i in range(5):
    	print(sess.run(one_element))

对DataSet中的元素做变换: Transformation

map:  dataset.map(lambda x:x+1)
batch: dataset.batch(32) 将每个元素组成了大小为32的batch
	batch的意思是将原始数据分成每个batch大小为batch_size的数据

shuffle: dataset.shuffle(buffer_size=10000),表示打乱时使用的buffer大小

repeat: 将整个序列重复多次，主要处理机器学习中的epoch，假设原先的数据是一个epoch，使用repeat(5)之后编程5个epoch

dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0,6.0]))
dataset = dataset.map(lambda x:x*2)
dataset = dataset.shuffle(buffer_size=10)
dataset = dataset.batch(3)
iterator = dataset.make_one_shot_iterator()
one_element = iterator.get_next()
with tf.Session() as sess:
	print(sess.run(one_element))
	print(sess.run(one_element))

[  6.   2.  10.]
[  8.   4.  12.]

如果不调用shuffle，那么repeat的是一样的数据
[ 2.  4.  6.]
[  8.  10.  12.]
[ 2.  4.  6.]
[  8.  10.  12.]


其他Iterator
initializable_iterator:必须在使用前通过sess.sun()来初始化，使用initializble_iterator ，可以将placeholder带入iterator中，可以方便通过参数快读定义Iterator

dataset = tf.data.Dataset.from_tensor_slices(tf.range(start=0, limit=limit))

iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
	sess.run(iterator.initializer, feed_dict={limit: 10})
	for i in range(10):
  		value = sess.run(next_element)
  		assert i == value
</code></pre>
<p>tf.string_split(source,delimiter=' ',skip_empty=True)</p>
<pre><code>string=[&quot;hello world Jack niu&quot;]
sp=tf.string_split(string)

with tf.Session() as sess:
	sess.run(tf.global_variables_initializer())
	print(sess.run(sp).values)

[b'hello' b'world' b'Jack' b'niu']


dataset = tf.data.Dataset.from_tensor_slices(
        tf.constant([&quot;c c a&quot;, &quot;c a&quot;, &quot;d&quot;, &quot;f e a g&quot;]))

# src_dataset = src_dataset.map(lambda  x:tf.string_split([x]).values)
dataset = dataset.map(lambda x: tf.string_split([x]).values)
src_max_len=2
dataset = dataset.map(lambda x: x[:src_max_len])
dataset = dataset.map(lambda src: (src, tf.size(src)))
iterator = dataset.make_one_shot_iterator()


one_element = iterator.get_next()
with tf.Session() as sess:
	print(sess.run(one_element))
	print(sess.run(one_element))
	print(sess.run(one_element))
	print(sess.run(one_element))

(array([b'c', b'c'], dtype=object), 2)
(array([b'c', b'a'], dtype=object), 2)
(array([b'd'], dtype=object), 1)
(array([b'f', b'e'], dtype=object), 2)
</code></pre>
<p>padded_batch:</p>
<pre><code>dataset =dataset.padded_batch(batch_size,padded_shape,padded_value)

许多模型（比如：序列模型）的输入数据的size多种多样（例如：序列具有不同的长度）为了处理这种情况，Dataset.padded_batch() 转换允许你将不同shape的tensors进行batch，通过指定一或多个dimensions，在其上进行pad

将顺序的元素组装成batch，
(array([b'c', b'c', b'a'], dtype=object), 3)
(array([b'c', b'a'], dtype=object), 2)
.....

这样的单一顺序元素
现在进行padded_batch(3,padded_shape=(tf.TensorShape[None],tf.TensorShape([])),padded_values=(&quot;UNK&quot;,0))

(array([[b'c', b'c', b'a'],
   [b'c', b'a', b'UNK'],
   [b'd', b'UNK', b'UNK']], dtype=object), array([3, 2, 1]))
(array([[b'f', b'e', b'a']], dtype=object), array([3]))

现在所有batch的长度都一样，就将所有长度不一的元素组成了一个batch内长度相同的元素。这样对这一个batch内的元素可以进行训练。

这种填充会对每一部分都进行pad，如果不进行pad，那就shape=[],value=0
</code></pre>
<p>示例：</p>
<pre><code>import tensorflow as tf

dataset = tf.contrib.data.Dataset.range(100)
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))
dataset = dataset.padded_batch(32, padded_shapes=[None],padding_values=(tf.cast(-1,tf.int64)))

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

sess= tf.Session()
print(sess.run(next_element))  # ==&gt; [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]
print(sess.run(next_element)[0])     # print(sess.run(one_element))

[31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31]]
[32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]
</code></pre>
<p>tf.logical_and(tf.size(src) &gt; 0, tf.size(tgt) &gt; 0))</p>
<p>src_tgt_dataset = src_tgt_dataset.shard(2, 1)</p>
<pre><code>shard: 将一个数据集划分成1/num_shard 的数据子集
索引代表需要哪一块数据。
</code></pre>
<p>padd</p>
<pre><code>数据是
(array([-1, -1,  0]), array([4, 2, 2]), array([2, 2, 3]))
(array([2, 2, 0]), array([4, 0, 1]), array([0, 1, 3]))
(array([2, 0]), array([4, 1, 2]), array([1, 2, 3]))

这里的注释:
# Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)


batched_dataset = src_tgt_dataset.apply(
    tf.contrib.data.group_by_window(
        key_func=key_func, reduce_func=reduce_func, window_size=batch_size))
这个函数有点麻烦。
</code></pre>
<p>tf.container(scope or &quot;train&quot;)</p>
<pre><code>有什么作用
</code></pre>
<p>tf.clip_by_global_norm(gradients, max_gradient_norm)</p>
<p>model 中的build_decoder</p>
<pre><code>iterator = self.iterator
print(&quot;iterator.source_sequence_length&quot;,iterator.source_sequence_length)
# maximum_iteration: The maximum decoding steps.
maximum_iterations = self._get_infer_maximum_iterations(
    hparams, iterator.source_sequence_length)

[3 2]
3
6
maximum_iterations =  6
</code></pre>
<p>dynamic_rnn 中的time_major</p>
<pre><code>time_major: The shape format of the inputs and outputs 
Tensors. If true, these Tensors must be shaped 
[max_time, batch_size, depth]. If false, these Tensors 
must be shaped [batch_size, max_time, depth]. Using 
time_major = True is a bit more efficient because it 
avoids transposes at the beginning and end of the RNN 
calculation. However, most TensorFlow data is batch-
major, so by default this function accepts input and 
emits output in batch-major form.
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Seq2Seq训练过程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Seq2Seq训练过程/" itemprop="url">Seq2Seq训练过程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T14:22:06+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Seq2seq训练过程</h2>
<p>iterator_utils.py</p>
<pre><code>class BatchedInput(collections.namedtuple(
		&quot;BatchedInput&quot;,(&quot;initializer&quot;, &quot;source&quot;, &quot;target_input&quot;,
        &quot;target_output&quot;, &quot;source_sequence_length&quot;,
        &quot;target_sequence_length&quot;))):
pass
collection.namedtuple: 生成可以使用名字来访问元素内容的tuple子类


def get_infer_iterator(src_dataset,
                   src_vocab_table,
                   batch_size,
                   eos,
                   src_max_len=None):

(src_ids, src_seq_len) = batched_iter.get_next()
return BatchedInput(
    initializer=batched_iter.initializer,
    source=src_ids,
    target_input=None,
    target_output=None,
    source_sequence_length=src_seq_len,
    target_sequence_length=None)

src_ids: 训练语句src中word在词汇表中的id
src_seq_len: 训练语句src本身的长度。

训练数据:
encoder: [&quot;f e a g&quot;, &quot;c c a&quot;, &quot;d&quot;, &quot;c a&quot;]
decoder: [&quot;c c&quot;, &quot;a b&quot;, &quot;&quot;, &quot;b c&quot;]

encoder_input: [2 0 3] c a eos：用eos pad，实际长度是2
				[-1 -1 0] -1代表unknown，句子长度为4，实际为3

decoder_input: [[4,2,2],[4,1,2]] [[sos c c],[sos b c]]
				输入用sos开始

decoder_target: [[2,2,3],[1,2,3]],[[c c eos],[b c eos]]	输出用eos pad

看一下具体的构造过程

# Create a tgt_input prefixed with &lt;sos&gt; and 
# a tgt_output suffixed with &lt;eos&gt;.
src_tgt_dataset = src_tgt_dataset.map(
    lambda src, tgt: (src,
                      tf.concat(([tgt_sos_id], tgt), 0),
                      tf.concat((tgt, [tgt_eos_id]), 0)),
    num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)
</code></pre>
<p>vocab_utils</p>
<pre><code>下载数据，加载embedding数据，这种数据可以通过别的其他的完备的
训练语料库来决定。
现在在这里并没有加载embedding 数据。
</code></pre>
<p>nmt_utils</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Seq2Seq基本模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Seq2Seq基本模型/" itemprop="url">Seq2Seq基本模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T13:59:23+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Seq2Seq中的模型</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Attention-Model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Attention-Model/" itemprop="url">Attention Model</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T13:59:04+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/02/02/Google-s-Neural-Machine-Translation-System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/02/Google-s-Neural-Machine-Translation-System/" itemprop="url">Google's Neural Machine Translation System</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-02T11:27:18+08:00">
                2018-02-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>谷歌机器翻译系统</h2>
<p>机器翻译是一个端对端学习方法。NMT系统在训练和翻译推断的方面都很昂贵，有时候需要很大的数据和很大的模型。GNMT模型包含8个encoder和8个decoder的LSTM网络，使用residual连接和attention连接在encoder和decoder之间。为了提高对稀有单词的处理，将单词分成一个有限的通用sub-word units(wordpieces)。</p>
<p>This method provides a good balance between the flexibility of &quot;character&quot;-delimited models and the efficiency of &quot;word&quot;-delimited models,naturally haandles translation of rare words,and ultimately improves the overall acuracy of the system.</p>
<p>Our beam search technique employs a length-normalization procedure and uses a coverage penalty,which encourages generation of an output sentence that is most likely to cover all the words in the source sentence.</p>
<p>To directly optimize the translation BLEU scores, we consider refining the modlels by using reinforcement learning, but we found that the improvment in the BLEU scores did not reflect in the human evaluation.</p>
<p>On the WMT'14 English-to-French and English-to-German benchmarks,GN&lt;T achieves competitive results to start-of-the-art.</p>
<p>Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors b an average 60% compared to Google's phrased-based production system</p>
<h2>1 Introduction</h2>
<p>Its architecture typically consists of two recurrent neural networks(RNNS),one to consume the input text sequence and one to generate translated output text. NMT is often accompanied by an attention mechanism which helps it cope effectively tirh long input sequences.</p>
<h2>2 Realted Work</h2>
<h2>3 Model Architecture</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">JackNiu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">Tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JackNiu</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
