<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="SpeechAndLanguageProcessing">
<meta property="og:url" content="http://niuwenchen.github.io/page/4/index.html">
<meta property="og:site_name" content="SpeechAndLanguageProcessing">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SpeechAndLanguageProcessing">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://niuwenchen.github.io/page/4/"/>





  <title>SpeechAndLanguageProcessing</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SpeechAndLanguageProcessing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">translate and learning language model</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/31/8-Neural-Networks-and-Neural/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/31/8-Neural-Networks-and-Neural/" itemprop="url">8 Neural Networks and Neural</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-31T09:56:40+08:00">
                2018-01-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Neural Networks and Neural Language Models</h2>
<p>本章将介绍许多其他方面的神经模型。</p>
<p>##Units
神经网络的最基本单元是一个计算单元。一个神经单元可以表示为</p>
<p>$$Z=b+ \sum_i w_{i}x_{i}$$</p>
<p>将求和过程替换为点乘过程</p>
<p>$$Z=w\cdot x+b$$</p>
<p>神经网络单元应用一个非线性函数给z。将这个函数的输出称为activation。</p>
<p>$$y=a=f(z)$$</p>
<p>三种非线性函数(sigmoid, tanh,relu)</p>
<p>sigmoid激活函数</p>
<p>$$y=\sigma (z) = \frac{1}{1+e^{-z}}$$</p>
<p>图像再不画，但是输出值的范围是[0,1]之间。对于输出限定在0和1之间的函数很有优势。但是这个函数在优化的过程中特别麻烦。</p>
<p><img src="/img/nlp8_0.png" alt=""></p>
<p><img src="/img/nlp8_1.png" alt=""></p>
<p>进行一个简单的例子</p>
<pre><code>w=[0.2,0.3,0.9]
b=0.5
x=[0.5,0.6,0.1]
</code></pre>
<p>$$y=\sigma (z) = \frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(0.5<em>0.2+0.3</em>0.6+0.1*0.8+0.5)}}=e^{-0.86}=0.42$$</p>
<p>tanh 激活函数，是sigmoid的变体,范围在-1 到1 之间</p>
<p>$$y=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$</p>
<p>最简单的激活函数relu函数</p>
<p>$$y=max(x,0)$$</p>
<p><img src="/img/nlp8_2.png" alt=""></p>
<p>这些激活函数有不同的属性，因此在不同的语言应用或者网络结构中有不同的使用方式。</p>
<h2>The XOR problem</h2>
<p>逻辑问题，首先出现在感知机中</p>
<p><img src="/img/nlp8_3.png" alt=""></p>
<pre><code>y:
0; if wx+b &lt;0
1; if wx=b &gt;0
</code></pre>
<p>可以用很简单的方式建立一个感知机</p>
<p><img src="/img/nlp8_4.png" alt=""></p>
<p>但是，没有办法建立一个感知机去解决逻辑XOR问题。</p>
<p>感知机是一个线性分类器。</p>
<p>。。。。。待补充</p>
<h2>Feed-Forward Neural Networks</h2>
<p>前馈神经网络。一个前向神经网络是一个多层网络但是没有环。前向神经网络也叫多层感知机。</p>
<p>最简单的前向神经网络如下图所示:</p>
<p><img src="/img/nlp8_5.png" alt=""></p>
<p>度量输出结果最好的方式是分类，常用的方式是归一化。softmax函数</p>
<p>$$y=softmax(x)$$</p>
<h2>Training Neutal Nets</h2>
<p>训练神经网络模型，就是对里面的参数进行调整，先对参数进行基本假设，然后在一个方向调整权重提高系统。</p>
<h3>Loss function</h3>
<p>神经网络的目标是定一个损失函数，然后寻找某种方式去最小化这个损失函数。
$$L(\hat{y},y)=How\ much\ \hat(y)\ differ\ from\ true\ y$$</p>
<p>损失函数---&gt;MSE(mean-squared error)</p>
<p>公式不再定义，就是误差平方均值。</p>
<p>多数情况下，我们考虑网络是一个概率分类。对于概率分类的误差函数是 cross entropy loss，或者叫做负对数似然(negative log likelihood)</p>
<p>Let y be a vector over the C classes representing the true output probability distribution. Assume this is a hard classification task,meaning that only one class is the correct one. If the true class is i, then y is a vector where yi=1 and yj=0. A vector like this,with one value =1 and the rest 0,is called one-hot vector. Noe let $\hat{y}$ be the vector output from the network. The loss is simply the log probability of the correct class：</p>
<p>$$L(\hat{y},y)= -logp(\hat{y})$$</p>
<h3>Following Gradients</h3>
<p>梯度下降方法
<img src="/img/nlp8_6.png" alt="">
<img src="/img/nlp8_7.png" alt=""></p>
<h3>Computing the Gradient</h3>
<p>计算梯度是用反向传播算法</p>
<h3>Stochastic Gradient Descent</h3>
<p><img src="/img/nlp8_8.png" alt="">
随机梯度下降之所以被称为随机是因为它选择一个随机样本，根据公式计算参数值。还有可选的算法是minibatch 梯度下降。</p>
<h2>Neural Language Models</h2>
<p>一个前向神经网络语言模型，时刻t的输入是一个前面的单词(wt-1,wt-2...)输出是一个对可能的下一个单词的概率分布。</p>
<p>$$P(w_{t}|w^{t-1}<em>1)\approx P(w</em>{t}|w^{t-1}_{t-N+1})$$
计算整个之前的上下文概率近似于计算之前的前N个单词的概率</p>
<p>如果我们使用一个4-gram，概率$P(w_{t}=i|w_{t-1},w_{t-2},w_{t-3})$</p>
<h3>Embeddings</h3>
<p>Embedding 过程
<img src="/img/nlp8_9.png" alt=""></p>
<p>经常是不会单独训练embeddings的，而是伴随训练网络的过程。</p>
<p>现在展示一个结构允许embeddings可以被学习。为了实现这个功能，增加一层。</p>
<p><img src="/img/nlp8_10.png" alt=""></p>
<ul>
<li>Select three embeddings from E:</li>
<li>Multiply by W:</li>
<li>Multiply by U:</li>
<li>Apply softmax: After the sofrmax,each node i in the output layer estimates the probability $P(w_{t}=i|w_{t-1},w_{t-2},w_{t-3})$</li>
</ul>
<p>如果我们使用e代表投影层，一个神经网络语言模型如下：</p>
<p>$$e=(Ex_{1},Ex_{2},...Ex)$$
$$h=\sigma (Wx+b)$$
$$z= Uh$$
$$y=softmax(z)$$</p>
<h3>Training the neural language model</h3>
<p>参数 $\theta =E,W,U,b$</p>
<p>对于一个句子开始训练，在每一个单词，计算交叉熵(negative log likelihood)损失是:</p>
<p>$$L=-logp(w_{t}|w_{t-1},...,w_{t-n+1})$$</p>
<p>梯度计算:</p>
<p>$$\theta_{t+1}=\theta_{t}-\eta \frac{\partial logp(w_{t}|w_{t-1},...,w_{t-n+1})}{\partial \theta }$$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/30/Localization-Overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/30/Localization-Overview/" itemprop="url">Localization Overview</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-30T18:57:07+08:00">
                2018-01-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Total Probability</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/24/R-Basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/24/R-Basics/" itemprop="url">R Basics</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-24T19:13:24+08:00">
                2018-01-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>R Basics</h2>
<p>文本分析包； TextCat</p>
<p>图形包: ggplot2</p>
<pre><code>install.packages('ggplot2')
library(ggplot2)

install.packages('RColorBrewer')
library(RColorBrewer)

data(diamonds)

# create scatterplot of price vs .carat color 
qplot(data=diamonds,x=carat,y=price,color=cut)+scalar_color_brewer(palette='Accent')
</code></pre>
<p><img src="/img/dv_01.png" alt=""></p>
<p><a href="https://www.statmethods.net/" target="_blank" rel="noopener">quick R</a>
<a href="http://www.cookbook-r.com/" target="_blank" rel="noopener">R Cookbook</a></p>
<p>getwd()获取工作目录</p>
<p>读取csv: read.csv(&quot; &quot;)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/24/Course-Information/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/24/Course-Information/" itemprop="url">Course Information</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-24T19:03:18+08:00">
                2018-01-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Course Information</h2>
<p>学习资源</p>
<ul>
<li><a href="http://swirlstats.com/students.html" target="_blank" rel="noopener">swirl</a>:Interactive mini-courses in R (great for beginners)</li>
<li><a href="https://www.rdocumentation.org/packages/DescTools/versions/0.99.19" target="_blank" rel="noopener">Listing of commands for R</a></li>
<li><a href="https://www.datacamp.com/community/tutorials/r-data-import-tutorial" target="_blank" rel="noopener">Data import tutorial</a></li>
<li><a href="https://www.statmethods.net/management/reshape.html" target="_blank" rel="noopener">Reshaping data</a></li>
<li>robustHD:
<ul>
<li><a href="http://www.nicebread.de/installation-of-wrs-package-wilcox-robust-statistics/" target="_blank" rel="noopener">Loading robustHD</a></li>
<li><a href="https://cran.r-project.org/web/packages/robustHD/robustHD.pdf" target="_blank" rel="noopener">Using robustHD</a></li>
<li><a href="https://www.rdocumentation.org/packages/robustHD/versions/0.5.1" target="_blank" rel="noopener">Reference for robustHD</a></li>
</ul>
</li>
</ul>
<p>依赖环境</p>
<p>R Datasets: iris,faithful,mtcars</p>
<p>To loda iris flower dataset(iris),</p>
<pre><code>data(iris)
head(iris)
levels(iris$Species)
data(faithful)
data(mtcars)
</code></pre>
<p>Libraries</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/23/7-Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/23/7-Logistic-Regression/" itemprop="url">7 Logistic Regression</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-23T20:26:08+08:00">
                2018-01-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Logistic Regression</h2>
<p>介绍第二个分类算法叫做多多项逻辑回归，一些涉及到语言处理的叫做最大熵模型。贝叶斯分类器和逻辑回归最大的不同之处是逻辑回归是一个discriminative(判别式)模型，而贝叶斯是一个generative(生成式)模型。</p>
<pre><code>根据贝叶斯公式，选择类别y是一个间接的过程，因此是一个生成式模型: 模型被训练从类别y中产生数据x。P(x|y)给定y，选择x。
而判别式模型一般是直接计算P(y|x)
</code></pre>
<p>$$\hat{y}=\underset{y}{argmax}P(y|x)$$</p>
<p>根据推导过程，并结合线性函数和归一化过程。</p>
<p>$$P(c|x) = \frac{exp(\sum_{i=1}{w_{i}f_{i}(c,x)})}{\sum_{c' \in C} (\sum_{i=1} w_{i}f_{i}(c',x))}$$</p>
<pre><code>其中fi是一个实值函数，在语言处理中一般使用二指特征，对每一个特征都有一个权重值，但是特征值是0还是1需要根据具体的输入数据进行判定。
</code></pre>
<p>逻辑回归就是在线性回归的基础上应用logit函数，然后得出的结果就是P(y=true|x)的概率</p>
<h2>Features in Multinomial Logistic Regression</h2>
<p>假定文本分类问题，需要知道是否将情绪标记为class +，-；
<img src="/img/nlp6_8.png" alt=""></p>
<pre><code>注意在lr中，一般定义特征函数为二元值，0，1分布。
</code></pre>
<h2>Classification in Multinomial Logistic Regression</h2>
<p><img src="/img/nlp6_9.png" alt="">
根据上述图片的描述，P(+|x)和P(-|x)可以被Eq.8.11 计算</p>
<p><img src="/img/nlp6_10.png" alt=""></p>
<p>如果文本的目标是做分类，那就不需要用上面的公式，直接使用下面的公式，也就是之前的贝叶斯公式即可。
<img src="/img/nlp6_11.png" alt=""></p>
<p>上面公式的索引从0-N,是查看所有的特征，但是实际上只需要看非零特征就可以了。</p>
<h2>Learning Logistic Regression</h2>
<p>LR训练方式是通过条件最大似然估计方式。选择参数w最大化y的概率，在给定数据x的情况下。</p>
<p>$$\hat{w}=\underset{c \in C}{argmax}logP(y^{j}|x^{j})$$</p>
<p>对于整个训练集就是上面右半部分的求和过程</p>
<p>$$\hat{w}=\underset{c \in C}{argmax}\sum _{j}logP(y^{j}|x^{j})$$</p>
<p><img src="/img/nlp6_12.png" alt=""></p>
<pre><code>上面的公式:
j代表每一个句子
i代表每一个特征，fi()代表每一个特征的特征函数，
j=1
	求和: i&lt;1~N; wi*fi(yj|xj) ，这里的yj是已知的。
	分母； 归一化过程，先令 y'=class1; 也就是每一个yj都需要
	将所有的class 遍历一遍。
</code></pre>
<p>求解权重过程变成了一个优化函数</p>
<p><img src="/img/nlp6_13.png" alt=""></p>
<p>左边就是数特征fk为1的次数(观察次数)，右边是期望的次数。</p>
<p>$$L'(w)=\sum_{j} Observed \ count(f_{k}) - Expected\  scount(f_{k})$$</p>
<h2>Regularization</h2>
<p>如果一个特征完美预测了结果只是因为它仅仅出现在一个类中，将会被设定一个非常高的权重。这种问题叫做overfitting 过拟合。</p>
<p>避免overfitting一个正则化的方法是增加一个函数。</p>
<p>$$\hat{w}=\underset{w}{argmax} \sum_{j} logP(y^{j}|x^{j})-\alpha R(w)$$</p>
<p>R(w)，正则化项，为了惩罚大的权重。</p>
<p>因此如果权重完美的匹配了训练数据，但是用很多有较大值的权重，将会被惩罚，相比于那些匹配训练数据了，但是效果不是最好的，但是使用了较小值的权重而言。</p>
<p>L2 regularization 是权重的二次方，称为欧几里德距离。</p>
<p>$$R(W) = ||W||^2_{2} = \sum_{j=1}^n w^2_{j}$$</p>
<p>L1 regularization 是一个线性函数，曼哈顿距离。</p>
<p>$$R(W) = ||W||<em>{1} = \sum</em>{i=1}^n |w_{i}|$$</p>
<p>这些正则化的类型来自于统计学，L1正则化叫做lasso 回归，L2正则化叫做ridge regression。两者都经常用在语言处理追踪。L2正则化更加容易去优化，简单的求导(2w),l1正则化更加复杂点。但是L2权重向量更加偏向于小向量，l1权重向量偏向于大向量，但是许多值都是0。 因此，L1正则化会生成稀疏箱量矩阵，也就是更少的特征。</p>
<p>下面用贝叶斯公式来解释L2约束。</p>
<p>L1和L2都会有贝叶斯解释来约束权重的先验值。L1正则化可以看作一个拉普拉斯先验，L2正则化权重分布符合高斯分母当均值$\mu =0$。 高斯分布，离mean越远，概率越低。</p>
<p><img src="/img/nlp6_14.png" alt=""></p>
<p>如果将每一个权重的高斯先验，就是最大化下面的约束条件</p>
<p><img src="/img/nlp6_15.png" alt=""></p>
<h2>Feature Selection</h2>
<p>特征选择的基础是给每一个特征一个较好的度量值，排序，保留最好的一个特征。特征数目作为一个元参数可以在dev set中训练。</p>
<p>特征排序是基于它们对分类决策提供了多少信息，最常用的方法是信息增益。信息增益是告诉我们这个单词表现多少位的信息帮助我们猜测类别，可以用如下的方式计算。</p>
<p>$$G(w)=-\sum^C_{i=1}P(c_{i})logP(c_{i})+P(w)\sum^C_{i=1}P(c_{i}|w)logP(c_{i}|w)+P(\bar{w})\sum^C_{i=1}P(c_{i}|\bar{w})log(P(c_{i}|\bar{w}))$$</p>
<pre><code>（1）原始数据 先按照类别c进行分类，计算熵
 遍历c
	计算条件概率的熵P(ci|w)logP(ci|w),w是文档中出现的单词
	计算不是该文档中该单词的熵。
</code></pre>
<p>特征选择对于没有正则化的类来说是非常重要的。</p>
<h2>Choosing a classifier and features</h2>
<p>NB分类器在小数据集和短文档中表现的更好，但是lr和svm在大数据集中表现的更好。</p>
<h2>Summary</h2>
<p>本章介绍逻辑回归(最大熵)模型。</p>
<ul>
<li>Multinomial logistic regression(also called MaxEnt or the Maximum Entropy classifier in language processing) is a discriminative model that assigns a class to an observation by computing a probability from an exponential function of a weighted set of feature of the observation</li>
<li>Regularization is important in MaxEnt models for avoiding overfitting</li>
<li>Feature selection can be helpful in removing useless features to speed up training, and is also important in unregularized for avoiding overfitting.</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/20/HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/20/HMM/" itemprop="url">HMM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-20T12:13:22+08:00">
                2018-01-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>HMM</h2>
<h3>1.1 马尔科夫过程</h3>
<p>随机过程最早是统计物理学的数学方法，起源于对统计力学的研究。一个简单的随机过程可以用一个状态图来描述:
<img src="/img/nlp16.png" alt=""></p>
<p>以上状态转移图用3个要素来描述</p>
<ul>
<li>状态：指系统中可能出现或存在的状态，也就是图中包含字母的圆圈，随机过程中，表示了系统随机变量的最小数目: a,e,h,i,p,t</li>
<li>转移：是指当发生只i的那个事件并满足指定条件时，系统由一种状态转移到另一种状态，也就是图中带有数字的箭头。状态转移表示两个状态之间的转换关系，在随机过程中，是指某个状态转移到下一个状态的概率</li>
<li>约束：从一个状态发出的所有箭头的概率总和为1</li>
</ul>
<p>整个状态转移过程可以用转移矩阵来表示。
<img src="/img/nlp17.png" alt="">
T表示当前状态，T+1表示下一时间状态。</p>
<h3>马尔可夫链及其概念</h3>
<p>马尔科夫过程是随机过程的一种，该过程具有如下特性：在已知系统当前状态的条件下，未来的演变与依赖过去的演变。也就是说，一个马尔科夫过程可以表示为系统在状态转移过程中，第T+1次结果只受第T次结果的影响，即只与当前状态有关，而与过去状态，即与系统的初始状态和此次转移前的所有状态无关。</p>
<p>下面给出形式化的定义。</p>
<p>设有随机过程<img src="/img/nlp18.png" alt="">,对于任意的整数<img src="/img/nlp19.png" alt="">，条件概率满足:
<img src="/img/nlp20.png" alt=""></p>
<p>称为马尔可夫链，简称马氏链。</p>
<p>马氏链的一步转移概率可以定义为:
<img src="/img/nlp21.png" alt=""></p>
<h3>应用举例</h3>
<p>任意相继两天中，雨天转晴天概率1/3,晴天转雨天1/2,0表示晴天状态，1表示雨天状态，一只5月1日为晴天，问5月3日为晴天，5月5日为雨天的概率
<img src="/img/nlp22.png" alt="">
则5月1日为晴天，5月3日为晴天的概率为5/12
继续求转移矩阵的4次方，即可得出雨天的概率为0.5995。</p>
<p>0时刻为已知状态，确定1 时刻的初始状态，确定一步转移矩阵，求2时刻的概率可以用 转移矩阵的平方。
<img src="/img/nlp23.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/18/6-Naive-Bayes-Classification-and-Sentiment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/18/6-Naive-Bayes-Classification-and-Sentiment/" itemprop="url">6 Naive Bayes Classification and Sentiment</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-18T16:44:48+08:00">
                2018-01-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>Naive Bayes  and Sentiment Classification</h2>
<p>贝叶斯是一个概率分类器，一个文本d，在所有类别C中，给出一个该文档属于某一个类别概率最大的类别。</p>
<p>$$ \widehat{c}=\underset{c\in C}{armax}P(c|d)=\underset{c\in C}{armax}\frac{P(d|c)P(c)}{P(d)} $$</p>
<p>化简该公式，P(d)概率对于每一个类都是一样的，对同一个文档d计算其最大概率，因此P(d)是一样的。</p>
<p>计算$\hat{c}$的过程: prior probabilities(先验概率)，P(d|c)的似然概率。</p>
<p>$$\widehat{c}=\underset{c\in C}{armax}\overset{likelihood}{P(d|c)}\overset{prior}{P(c)}$$</p>
<p>一个文档可以用一系列特征来替代</p>
<p>$$\widehat{c}=\underset{c\in C}{armax}\overset{likelihood}{P(f1,f2,fn|c)}\overset{prior}{P(c)}$$</p>
<p>对于上面的等式，如果没有一些简化假设，无法计算（对于每一个单词和位置都有可能）。</p>
<p>第一种方式是BOW假设：不考虑位置，只关注单词，无论love这个单词出现在di1位或者第20位都有相同的意义或影响力。因此，假设特征f1,f2,...fn仅仅编码单词的id，不考虑位置。</p>
<p>第二种方式通常被称为贝叶斯假设： 条件独立性假设，$P(f_{i}|c)$独立分布，并且上面的计算公式可以被假设为以下:</p>
<p>$$P(f_{1},f_{2},f_{n}|c)=P(f_{1}|c)P(f_{2}|c)...P(f_{n}|c)$$</p>
<p>最后的公式可以如下:</p>
<p>$$c_{NB}=\underset{c\in C}{argmax}P(c)\prod P(f|c)$$</p>
<p>为了将NB分类器应用到文本中，需要考虑单词位置，简单的给文档中的每一个单词给一个下标</p>
<p>positions &lt;--all word positions in ttest document；</p>
<p>NB分类器计算方式是对数形式，避免underflow和increase speed,</p>
<p>$$c_{NB}=\underset{c\in C}{argmax}logP(c)+\sum_{i\in positions}P(w_{i}|c)$$</p>
<p>通过在对数空间中计算特征，上面等式将输入特征作为一个线性函数来预测类别。 分类似使用一个线性组合计算分类---类似NB和LR等---都被称为线性分类器。</p>
<p>也就是说: 贝叶斯分类器最后是一个线性分类器。</p>
<h2>训练贝叶斯分类器。</h2>
<p>计算先验概率$P(c)$，</p>
<p>$\hat{P(c)}=\frac{N_{c}}{N_{doc}}$。</p>
<p>计算条件概率$P(f_{i}|c)$</p>
<p>假设每一个在文档中的单词是一个特征，因此计算的是 $P(w_{i}|c)$,</p>
<p>计算在所有文档中单词$w_{i}$作为某一个类别c出现的次数，</p>
<p>$$\hat{P(w_{i}|c)}=\frac{count(w_{i},c)}{\sum_{w\in V} count(w,c)}$$</p>
<p>V是所有类别中出现的词汇，不仅仅是在类别c中出现的词汇。</p>
<pre><code>这里有个疑问? 类别1，计算类别1中单词w出现的次数，分母是确定的，分母是计算所有类别1中所有单词出现的次数，而不仅仅是特定的w。
</code></pre>
<p>还需要对这个公式做一些平滑处理，最简单的方式是add-one 平滑方法。</p>
<p>$$\hat{P(w_{i}|c)}=\frac{count(w_{i},c)+1}{(\sum_{w\in V} count(w,c))+|V|}$$</p>
<p>一些系统选择完全忽略一些单词的类别: stop words,非常常见的单词如the和a。将词汇表按照顺序排序，定义top10~100的单词为定用词，或者选择一些已经定义好的停用词。然后这些停用词将从训练集和测试集中删除。</p>
<h2>Worked example</h2>
<p><img src="/img/nlp6_1.png" alt=""></p>
<p>计算过程
<img src="/img/nlp6_2.png" alt=""></p>
<pre><code>计算P(w|c)出现一些疑惑？
计算count(w,c)
for d in document_of_c:	
	count(w,c) 这里的计数过程是统计每一个文档中该单词出现的次数，还是统计出现该单词的文档次数。
	目前暂定为出现的单词次数。
</code></pre>
<p>上面的数据P(-)=3/5=0.6; P(+)=2/5=0.4</p>
<pre><code>计算label为- 中的单词条件概率，
	总的词汇是20个，negative中出现的单词是14个，分母是count(w,c)=14，对于该类不变。分子变化
	predictable： 1次
	no: 1
	fun: 1
计算label为+中的条件概率
	总的词汇是20个，positive中为9个，分母为9保持不变
	predictable： 0次
	no: 0
	fun:1
</code></pre>
<p><img src="/img/nlp6_3.png" alt=""></p>
<p>$$P(-)P(S|-)=3/5\times 2/34 \times 2/34 \times 1/34 = 6.1 \times 10^{-5}$$
$$P(+)P(S|+)=2/5\times 1/29 \times 1/29 \times 2/29 = 3.2 \times 10^{-5}$$</p>
<p>因此这个模型预测这个句子是class negative。</p>
<pre><code>从上面的计算过程可以看出，贝叶斯分类器实际上根本不用训练分类器，只是将测试数据
和训练数据用某种方式计算概率即可。
根据类别的定义，实际上在计算过程中，训练语料的先验概率，似然值中属于某一个类的
分母都是确定的，只是分子不同而已。
但是在这个例子中还是没有说明对于句子中出现相同词的计算方式，现在就按照词的次数
进行计算，如果在一个文档中出现多次，那就累加多次。
</code></pre>
<h2>Optimizing for Sentiment Analysis</h2>
<p>情感分类器中，一个单词是否出现的影响力远远高于它出现的频率。因此在情感分类器中，通过对一个文档中进行单词去重。</p>
<p><img src="/img/nlp6_4.png" alt=""></p>
<p>第一种方式是单词去重，对于情感分析而言，只要其出现就行，不要求数量</p>
<p>另一种方式是对于negation样本的处理。</p>
<pre><code>I really like this movie 
I didn't like this movie
</code></pre>
<p>负样本通过一个didn't 完全的将整个语境改变。因此，负样本完全可以通过修改一个单词使该样本变成正样本。</p>
<p>一种非常简单的方法在情绪分析中是将negation中的单词全部加上NOT_前缀，</p>
<pre><code>didn't like this movie, but I
didn't NOT_like NOT_this NOT_movie, but I
</code></pre>
<p>新形式大单词像NOT_like ,NOT_recommend 将会在负样本中出现多次，并且是negative情绪的关键指示，但是像NOT_bored,NOT_dismiss 将会表现为positive情绪。</p>
<p>sentiment lexicons（情绪字典），General Inquirer，LIWC，MPQA。</p>
<p>MPQA字典有6885个单词，2718个正例，4912个反例。</p>
<pre><code>+: admirable,beautiful,condident,dazzling,ecstatic,favor,glee,great
-: awful,bad,bias,catastroph,cheat,deny,envious,foul,harsh,hate
</code></pre>
<p>18章将会介绍如何自动分类这些单词。</p>
<h2>Naive Bayes as a Language Model</h2>
<p>贝叶斯分类可以使用任何特征:字典，url，email，网络特征，短语，解析树等等。但是如果按照前面介绍的，仅使用独立的单词特征，使用在文档中的所有特征，贝叶斯分类器和LM模型很相似。特别的，一个贝叶斯分类器可以被看作一个限定类别的unigram 语言模型，对于每一个类别而言都会生成一个语言模型</p>
<p>对于每一个单词,概率 P(word|c), 这个模型也会对每一个句子赋予一个概率</p>
<p>$$P(s|c)=\prod <em>{i \in sentence} P(w</em>{i}|c)$$</p>
<p><img src="/img/nlp6_5.png" alt=""></p>
<pre><code>P(&quot;I love this fun film&quot;|+)= 0.1*0.1*0.01*0.05*0.1 = 0.0000005
P(&quot;I love this fun film&quot;|-)=...=0.0000000010

P(s|pos) &gt;P(s|neg).
</code></pre>
<h2>Evaluation:Precision,Recall,F-measure</h2>
<p><img src="/img/nlp6_6.png" alt=""></p>
<p>accuracy有局限，很简单的例子，</p>
<pre><code>假如100万封邮件，只有100封邮件在表达喜欢或讨厌
假设一个分类器将所有的邮件都标成&quot;不谈论公司&quot;。这个分类器将会有999900个正确的分类并且只有
100个错误分类，那么准确率是99.99%。
准确率很高，但是实际上这个分类器是无效的。
准确性度量指标对于发现一些特殊的，或者不均衡的样本来说不是最好的、
Precision: 度量系统认为正样本中，被判断准确的概率（以系统为衡量基准）
</code></pre>
<p>$$Precision = \frac{true \ positives}{true \ positiive + false\ positives}$$</p>
<pre><code>Recall: measures the percentage of items actually present in the input that were identified by the system
度量所有正样本被判断准确的概率，（以原始数据为衡量基准）
</code></pre>
<p>$$Recall = \frac{true \ positives}{true \ positiive + false\ negatives}$$</p>
<p>实际中，将Precision和Recall结合起来使用，F-measure</p>
<p>$$F_{\beta } =\frac{(\beta ^2+1)PR}{\beta ^2P+R}$$</p>
<p>F-measure度量有个倾向性，$\beta &gt;1$ 倾向于recall，小于1 倾向于precision，相等的话则两者均衡。F1度量方式。</p>
<p>$$F_{1} = \frac{2PR}{P+R}$$</p>
<h2>More than two classes</h2>
<h2>Test sets and Cross-validation</h2>
<p>用training训练模型，用devset调整参数，用test生成性能报告。</p>
<p>cv: 随机选择训练集和测试集，训练模型，然后计算错误率。然后重复随机选择数据。重复10次会得到10次错误率， 10折交叉验证法。</p>
<p><img src="/img/nlp6_7.png" alt=""></p>
<h2>Statistical Significance Testing(统计显著性检验)</h2>
<h2>总结</h2>
<p>介绍贝叶斯分类器，在情绪分类中使用。</p>
<ul>
<li>Many language processing tasks can be viewed as tasks of classification. learn to model the class given the observation</li>
<li>Text categorization,in which an entire text is assigned a class from a finite set, comprises such tasks as sentiment analysis,spam detection,email classification,and authorship attribution.</li>
<li>Sentiment analyssis classifis a text as reflecting the positive or negative orientation that a weiter expresses toward some object</li>
<li>Naive Bayes is a generative model that make the bag of words assumption and conditional independence assumption</li>
<li>Naive Bayes with binarized features seems to work better for many text classificatoin tasks</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/18/5-Spelling-Correction-and-the-Noisy-Channel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/18/5-Spelling-Correction-and-the-Noisy-Channel/" itemprop="url">5 Spelling Correction and the Noisy Channel</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-18T14:46:35+08:00">
                2018-01-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>##	Spelling Correction and the Noisy Channel
主要介绍拼写检查和改正的问题。拼写检查从两方面来看，Non-word spelling correction(非单词拼写纠正)是检查和修正拼写错误导致不存在的单词(graffe giraffe)。real word spelling correction是关于真实存在单词的检查和纠正的任务，可能由于意外的发生(插入，删除，替换)导致一个另外的单词(there three). cognitive errors(认知错误)是作者使用同音词导致错误拼写(dessert--&gt;desert, piece =&gt;peace)</p>
<p>Non-Word errors: 检查字典可以完成</p>
<pre><code>1. 生成正确拼写候选集: graffe-&gt;(giraffe,graf,grail)
2. 排序: 可以使用最小编辑距离算法
3. giraffe 更加接近于graffe
</code></pre>
<p>Real-word spelling error: 更加困难，噪声信道发现候选集，进而排序</p>
<h3>The Noisy Channel Model</h3>
<p>噪声信道模型应用于拼写改正任务。</p>
<p><img src="/img/nlp08.png" alt="">
噪声信道把错误拼写的单词当作一个正确拼写的代词，只不过这个单词是经过一个噪声通信信道之后被扭曲了。这个信道通过将字母替换或其他方式引入噪声，难以识别正确单词。我们的目的是建立这样一个噪声信道，通过输入每一个单词到噪声模型以观察哪一个生成的单词是最接近错误的单词。</p>
<pre><code>贝叶斯定理
错误单词: y
输入单词: w
w= argmaxP(w|y), 产生结果y，最可能的输入w。
p(a|b) = p(b|a)p(a)/p(b)

则: w = argmax{p(y|w)p(w)/p(x)}
p(x)对于每个单词都是一样，w=argmax{p(y|w)p(w)}
p(y|w) 是信道模型，p(w)是先验模型
</code></pre>
<p>下面一个例子说明过程 acress</p>
<pre><code>function Noisy Channel Spelling(word x,dict D,lm,editprob) returns correction

if x not in D
	candidates,edits &lt;---All strings at edit distance 1 from x that are in D,and theri edit
	for each c,e in candidates,edits
		channel &lt;--- editprob(e)
		priot &lt;---lm(x)
		score[c] = log channel + log prior
	return argmax score[c]

发现相似单词：最简单的是查找编辑距离为1的单词
含交换的编辑距离是Damerau-Levenshtein 编辑距离算法
</code></pre>
<p><img src="/img/nlp09.png" alt=""></p>
<pre><code>已经找出所有的候选集，开始计算概率
P(w) 是语言模型概率，肯定是根据训练语料库统计候选集中出现单词的概率
</code></pre>
<p><img src="/img/nlp10.png" alt=""></p>
<pre><code>计算信道模型
一种最简单的估计模型，p(acress|across)使用字母e被用作字母o的出现次数的概率。为了使用这种方式需要混淆矩阵，4个。

del[x,y]:count(xy typed as x)
ins[x,y]:count(x typed as xy)
sub[x,y]:count(x typed as y)
trans[x,y]:count(xy typed as yx)

如何获取混淆矩阵? 从错误拼写中抽取

	additional: addional,additonal
	environments:envirnments,enviorments,enviroments
	preceded: preceeded
	....
</code></pre>
<p>错误拼写语料库
<a href="http://www.dcs.bbk.ac.uk/~ROGER/corpora.html" target="_blank" rel="noopener">http://www.dcs.bbk.ac.uk/~ROGER/corpora.html</a>,<a href="http://norvig.com/ngrams/" target="_blank" rel="noopener">http://norvig.com/ngrams/</a></p>
<pre><code>混淆矩阵建立算法:
迭代算法：
	初始化矩阵：相同值
	每一个字母都能被删除，替换，插入，交换等等
	拼写错误修正算法开始运行在一个拼写错误的数据集中；
	给定错别字集合和预测修正对，再次运算混淆矩阵，拼写算法再次运行....
	EM 算法(chapter 9)
</code></pre>
<p><img src="/img/nlp11.png" alt=""></p>
<p><img src="/img/nlp12.png" alt=""></p>
<p><img src="/img/nlp13.png" alt=""></p>
<p>across 是最好的修正，接下来是actress,但是原文：</p>
<pre><code>stellar and versatile acress whose combination of sass and glamour has defined her. . .
实际上需要的是actress，unigram模型出错，使用二元模型

P(actressjversatile) = :000021
P(acrossjversatile) = :000021
P(whosejactress) = :0010
P(whosejacross) = :000006

P(“versatile actress whose”) = :000021∗ :0010 = 210× 10−10
P(“versatile across whose”) = :000021∗ :000006 = 1× 10−10

将这个二元模型的概率和噪声模型概率相乘，得到的是正确的结果。
</code></pre>
<h3>Real-word Spelling errors</h3>
<p>使用噪声信道开始进行真实单词的拼写错误检查。</p>
<pre><code>输入句子: X={x1,x2,x3....xk,....xn}
生成候选集正确句子: C(X)
选择最高语言模型概率的句子作为结果

X= Only two of thew apples 
候选集:

only two of thew apples
oily two of thew apples
only too of thew apples
only to of thew apples
only tao of the apples
only two on thew apples
only two off thew apples
only two of the apples
only two of threw apples
only two of thew applies
only two of thew dapples

W = argmaxP(X|W)P(W)
</code></pre>
<p><img src="/img/nlp14.png" alt=""></p>
<pre><code>假设: two of thew
假设: thew 是错误的，候选集: the,thae,thew,threw,thwe
计算语言模型概率
P(the|two of) = 0.476012
P(thew|two of) = 9.95051 ×10−8
P(thaw|two of) = 2.09267 ×10−7
P(threw|two of) = 8.9064 ×10−7
P(them|two of) = 0.00144488
P(thwe|two of) = 5.18681 ×10−9

正确单词模型和错误单词模型不同，这里计算的是整个句子的概率，因此还要计算 two of the people 这个句子的概率，即P(people | of the),p(people|of thew)...

取alpha是0.05,p(w|w) = 0.95
</code></pre>
<p><img src="/img/nlp15.png" alt=""></p>
<p>对于这个错误的句子，模型选择了正确的句子，the</p>
<h3>Noisy Channel Model: The State of The Art</h3>
<p><a href="http://www.aclweb.org/anthology/C90-2036" target="_blank" rel="noopener">噪声信道模型</a>
<a href="https://www.youtube.com/watch?v=RgHr2KVXtiE" target="_blank" rel="noopener">视频</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/18/维特比分词/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/18/维特比分词/" itemprop="url">维特比分词</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-18T11:28:41+08:00">
                2018-01-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3>维特比算法理解</h3>
<p>维特比算法就是动态规划实现最短路径，只要知道“动态规划可以降低复杂度”这点即可。</p>
<pre><code>维特比算法是一个特殊但应用最广的动态规划算法，利用动态规划，可
以解决任何一个图中的最短路径问题。而维特比算法是针对一个特殊的
图——篱笆网络的有向图（Lattice )的最短路径问题而提出的。 它之
所以重要，是因为凡是使用隐含马尔可夫模型描述的问题都可以用它来
解码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词等。——《数学之美》 吴军
</code></pre>
<p>篱笆网络有向图的特点是同一列节点有多个，并且和上一列节点交错地连接起来。同一列节点代表同一个时间点上不同的状态的并列，大概因为这种一列一列整齐的节点和交错的边很像篱笆而得名。</p>
<p><img src="/img/nlp_a.png" alt=""></p>
<p>类似神经网络连接图<a href="http://playground.tensorflow.org" target="_blank" rel="noopener">神经网络游乐场</a></p>
<p>加上上图每一列分别有n1....nn个节点，那么计算复杂度就是O(n1*n2...nn),每条路径都走一遍</p>
<p>而维特比算法的精髓就是，既然知道到第i列所有节点Xi{j=123…}的最短路径，那么到第i+1列节点的最短路径就等于到第i列j个节点的最短路径+第i列j个节点到第i+1列各个节点的距离的最小值。</p>
<p>复杂度: 假设篱笆有向图中每一列节点最多有D个，一共有N列，那么，每次计算最多D*D次，最多计算N次，复杂度O(NDD),远远小于O(D^N)。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://niuwenchen.github.io/2018/01/18/Wordnet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="JackNiu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SpeechAndLanguageProcessing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/18/Wordnet/" itemprop="url">Wordnet</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-18T09:29:02+08:00">
                2018-01-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2>WordNet介绍</h2>
<h3>1.1 简介</h3>
<p>WordNet是一个由普林斯顿大学认识科学实验室在心理学教授乔治·A·米勒的指导下建立和维护的大型的英语词典，WordNet的开发有两个目的：</p>
<pre><code>1. 它既是一个字典，又是一个辞典，它比单纯的辞典或词典都更加易于使用。
2. 支持自动的文本分析以及人工智能应用。
</code></pre>
<p>WordNet 将英语的名词、动词、形容词、和副词组织为(同义词网络)Synsets，每一个Synset表示一个基本的词汇概念，并在这些概念之间建立了包括同义关系（synonymy）、反义关系（antonymy） 、上下位关系（hypernymy &amp; hyponymy） 、部分关系（meronymy）等多种语义关系。</p>
<h3>NLTK WordNet Interface</h3>
<pre><code>from nltk.corpus import wordnet as wn
</code></pre>
<p><strong>Words</strong></p>
<pre><code>Look up a word using synsets(); this function has an
optional pos argument which lets you constrain the 
part of speech of the word:

&gt;&gt;&gt; wn.synsets('dog')
[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'),
Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]
&gt;&gt;&gt; wn.synsets('dog', pos=wn.VERB)
[Synset('chase.v.01')]

The other parts of speech are NOUN, ADJ and ADV. A synset is identified with a 3-part name of the form: word.pos.nn:

&gt;&gt;&gt; wn.synset('dog.n.01')
Synset('dog.n.01')
&gt;&gt;&gt; print(wn.synset('dog.n.01').definition())
a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds

&gt;&gt;&gt; len(wn.synset('dog.n.01').examples())
1
&gt;&gt;&gt; print(wn.synset('dog.n.01').examples()[0])
the dog barked all night
&gt;&gt;&gt; wn.synset('dog.n.01').lemmas()
[Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')](家犬)

&gt;&gt;&gt; [str(lemma.name()) for lemma in wn.synset('dog.n.01').lemmas()]
['dog', 'domestic_dog', 'Canis_familiaris']
&gt;&gt;&gt; wn.lemma('dog.n.01.dog').synset()
Synset('dog.n.01')

The WordNet corpus reader gives access to the Open Multilingual WordNet, using ISO-639 language codes.

&gt;&gt; sorted(wn.langs())
['als', 'arb', 'cat', 'cmn', 'dan', 'eng', 'eus', 'fas','fin', 'fra', 'fre', 'glg', 'heb', 'ind', 'ita', 'jpn', 'nno','nob', 'pol', 'por', 'spa', 'tha', 'zsm']
&gt;&gt;&gt; wn.synsets(b'\xe7\x8a\xac'.decode('utf-8'), lang='jpn')
[Synset('dog.n.01'), Synset('spy.n.01')]
&gt;&gt;&gt; wn.synset('spy.n.01').lemma_names('jpn')
['\u3044\u306c', '\u307e\u308f\u3057\u8005', '\u30b9\u30d1	\u30a4', '\u56de\u3057\u8005',
'\u56de\u8005', '\u5bc6\u5075', '\u5de5\u4f5c\u54e1', '\u5efb\u3057\u8005',
'\u5efb\u8005', '\u63a2', '\u63a2\u308a', '\u72ac', '\u79d8\u5bc6\u635c\u67fb\u54e1',
'\u8adc\u5831\u54e1', '\u8adc\u8005', '\u9593\u8005', '\u9593\u8adc', '\u96a0\u5bc6']
&gt;&gt;&gt; wn.synset('dog.n.01').lemma_names('ita')
['cane', 'Canis_familiaris']
&gt;&gt;&gt; wn.lemmas('cane', lang='ita')
[Lemma('dog.n.01.cane'), Lemma('hammer.n.01.cane'), Lemma('cramp.n.02.cane'),
Lemma('bad_person.n.01.cane'), Lemma('incompetent.n.01.cane')]
&gt;&gt;&gt; sorted(wn.synset('dog.n.01').lemmas('dan'))
[Lemma('dog.n.01.hund'), Lemma('dog.n.01.k\xf8ter'),
Lemma('dog.n.01.vovhund'), Lemma('dog.n.01.vovse')]
&gt;&gt;&gt; sorted(wn.synset('dog.n.01').lemmas('por'))
[Lemma('dog.n.01.cachorro'), Lemma('dog.n.01.c\xe3es'),
Lemma('dog.n.01.c\xe3o'), Lemma('dog.n.01.c\xe3o')]
&gt;&gt;&gt; dog_lemma = wn.lemma(b'dog.n.01.c\xc3\xa3o'.decode('utf-8'), lang='por')
&gt;&gt;&gt; dog_lemma
Lemma('dog.n.01.c\xe3o')
&gt;&gt;&gt; dog_lemma.lang()
'por'
&gt;&gt;&gt; len(wordnet.all_lemma_names(pos='n', lang='jpn'))
66027
</code></pre>
<h3>Synsets</h3>
<pre><code>Synset: a set of synonyms that share a common meaning.
&gt;&gt;&gt; dog = wn.synset('dog.n.01')
&gt;&gt;&gt; dog.hypernyms()
[Synset('canine.n.02'), Synset('domestic_animal.n.01')]
&gt;&gt;&gt; dog.hyponyms()  # doctest: +ELLIPSIS
[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), ...]
&gt;&gt;&gt; dog.member_holonyms()
[Synset('canis.n.01'), Synset('pack.n.06')]
&gt;&gt;&gt; dog.root_hypernyms()
[Synset('entity.n.01')]
&gt;&gt;&gt; wn.synset('dog.n.01').lowest_common_hypernyms(wn.synset('cat.n.01'))
[Synset('carnivore.n.01')]
</code></pre>
<p>Each synset contains one or more lemmas, which represent a specific sense of a specific word.</p>
<p>Note that some relations are defined by WordNet only over Lemmas:</p>
<pre><code>&gt;&gt;&gt; good = wn.synset('good.a.01')
&gt;&gt;&gt; good.antonyms()
Traceback (most recent call last):
File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
AttributeError: 'Synset' object has no attribute 'antonyms'
&gt;&gt;&gt; good.lemmas()[0].antonyms()
[Lemma('bad.a.01.bad')]

good = wn.synset('good.a.01')
good.lemmas()
Out[40]: [Lemma('good.a.01.good')]
good.lemmas()[0]
Out[41]: Lemma('good.a.01.good')
good.lemmas()[0].antonyms()  反义词
Out[42]: [Lemma('bad.a.01.bad')]
</code></pre>
<h3>Lemmas</h3>
<pre><code>eat= wn.lemma('eat.v.03.eat')
Out[47]: Lemma('feed.v.06.eat')
eat = wn.lemma('eat.v.03.eat')
eat
Out[49]: Lemma('feed.v.06.eat')

print(eat.key())
eat%2:34:02::
</code></pre>
<h3>Verb Frams</h3>
<pre><code>&gt;&gt;&gt; wn.synset('think.v.01').frame_ids()
[5, 9]
&gt;&gt;&gt; for lemma in wn.synset('think.v.01').lemmas():
...     print(lemma, lemma.frame_ids())
...     print(&quot; | &quot;.join(lemma.frame_strings()))
...
Lemma('think.v.01.think') [5, 9]
Something think something Adjective/Noun | Somebody think somebody
Lemma('think.v.01.believe') [5, 9]
Something believe something Adjective/Noun | Somebody believe somebody
Lemma('think.v.01.consider') [5, 9]
Something consider something Adjective/Noun | Somebody consider somebody
Lemma('think.v.01.conceive') [5, 9]
Something conceive something Adjective/Noun | Somebody conceive somebody
</code></pre>
<h3>Similarity</h3>
<pre><code>dog = wn.synset('dog.n.01')
cat = wn.synset('cat.n.01')
hit = wn.synset('hit.v.01')
slap = wn.synset('slap.v.01')
dog.path_similarity(cat)
Out[62]: 0.2
hit.path_similarity(slap)
Out[63]: 0.14285714285714285
wn.path_similarity(hit, slap)
Out[64]: 0.14285714285714285
print(hit.path_similarity(slap, simulate_root=False))
None
</code></pre>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">JackNiu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">Artikel</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">Tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JackNiu</span>

  
</div>


  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
